{
  "paragraphs": [
    {
      "text": "%spark.pyspark\n\nimport numpy as np\nimport pandas as pd\nimport pyspark.ml as ml\nimport matplotlib.pylab as plt\nfrom collections import Counter\nimport pyspark.sql.functions as f\nfrom pyspark.ml.feature import VectorAssembler\nfrom pyspark.sql.functions import lit, col, when, floor\nfrom pyspark.ml.classification import MultilayerPerceptronClassifier\nfrom pyspark.ml.evaluation import MulticlassClassificationEvaluator",
      "user": "dcr",
      "dateUpdated": "2021-07-27 14:15:06.924",
      "config": {
        "colWidth": 12.0,
        "fontSize": 9.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "python",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "editorMode": "ace/mode/python"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": []
      },
      "apps": [],
      "jobName": "paragraph_1625581653395_1507700533",
      "id": "20210706-142733_1394646815",
      "dateCreated": "2021-07-06 14:27:33.395",
      "dateStarted": "2021-07-27 14:15:06.970",
      "dateFinished": "2021-07-27 14:15:07.775",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%spark.pyspark\n# from mllib_results import MLlib_confusion_matrix, MLlibMultiClassEvaluator, plottingThreshold",
      "user": "dcr",
      "dateUpdated": "2021-07-19 14:47:53.967",
      "config": {
        "colWidth": 12.0,
        "fontSize": 9.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "python",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "editorMode": "ace/mode/python"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": []
      },
      "apps": [],
      "jobName": "paragraph_1625839938596_994557036",
      "id": "20210709-141218_1170342984",
      "dateCreated": "2021-07-09 14:12:18.596",
      "dateStarted": "2021-07-19 14:47:55.458",
      "dateFinished": "2021-07-19 14:48:18.038",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%spark.pyspark\n\n# define the data source\n# gs_df \u003d sqlContext.read.parquet(\u0027file:////user/nch/PARQUET/TESTS/GEDR3/*.parquet\u0027)\n# # register as SQL-queryable \n# gs_df.createOrReplaceTempView(\u0027dcr_gaia_source\u0027)",
      "user": "dcr",
      "dateUpdated": "2021-07-27 13:25:57.915",
      "config": {
        "colWidth": 12.0,
        "fontSize": 9.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "python",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "editorMode": "ace/mode/python"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": []
      },
      "apps": [],
      "jobName": "paragraph_1625648950407_1021994287",
      "id": "20210707-090910_1022361690",
      "dateCreated": "2021-07-07 09:09:10.407",
      "dateStarted": "2021-07-27 12:51:56.719",
      "dateFinished": "2021-07-27 12:52:44.882",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "title": "Collect Required Data",
      "text": "%spark.pyspark\n# clear any previously cached data in the context (cells may be executed in any order, and out-dated by changes from here onwards)\nsqlContext.clearCache()\n\n# Select all data\nraw_sources_df \u003d spark.sql(f\u0027SELECT source_id, parallax, parallax_error,parallax_over_error,pmra,\\\nastrometric_sigma5d_max,pmdec,pmdec_error,pmra_error,astrometric_excess_noise,\\\nvisibility_periods_used,ruwe,astrometric_gof_al,ipd_gof_harmonic_amplitude,ipd_frac_odd_win,ipd_frac_multi_peak,phot_g_mean_mag,phot_rp_mean_mag, \\\ng_rp FROM gaia_source WHERE (1/parallax \u003c 0.1)\u0027)\n# cache it for speedy access below (all subsequent samples are derived from this):\n# raw_sources_cached \u003d raw_sources_df.cache()\n# ... some good advice concerning caching in Spark here: https://towardsdatascience.com/best-practices-for-caching-in-spark-sql-b22fb0f02d34\n\n# register as SQL-queryable\nraw_sources_df.createOrReplaceTempView(\u0027dcr_raw_sources\u0027)\n\n# raw_sources_df.count()\n# EDR3: 412,503,019 sources in 15min 26sec (\u003c3 kpcs - Possibly \u003e3kpcs)??\n# EDR3 :XX sources in XXmin XXsec (\u003c3 kpcs)\n# EDR3: XX sources in XXmin XXsec (\u003c1 kpcs)",
      "user": "dcr",
      "dateUpdated": "2021-07-27 15:49:56.607",
      "config": {
        "colWidth": 12.0,
        "fontSize": 9.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "python",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "editorMode": "ace/mode/python",
        "title": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": []
      },
      "apps": [],
      "jobName": "paragraph_1625581617886_-1521281747",
      "id": "20210706-142657_7677965",
      "dateCreated": "2021-07-06 14:26:57.886",
      "dateStarted": "2021-07-27 14:15:21.908",
      "dateFinished": "2021-07-27 14:15:24.039",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%spark.pyspark\nraw_sources_df.count()",
      "user": "dcr",
      "dateUpdated": "2021-07-27 15:29:53.283",
      "config": {
        "colWidth": 12.0,
        "fontSize": 9.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "python",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "editorMode": "ace/mode/python"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "ERROR",
        "msg": [
          {
            "type": "TEXT",
            "data": "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)\n\u001b[0;32m\u003cipython-input-18-5e7e12b06d63\u003e\u001b[0m in \u001b[0;36m\u003cmodule\u003e\u001b[0;34m\u001b[0m\n\u001b[0;32m----\u003e 1\u001b[0;31m \u001b[0mraw_sources_cached\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\n\u001b[0;32m/opt/spark/python/lib/pyspark.zip/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36mcount\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    522\u001b[0m         \u001b[0;36m2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    523\u001b[0m         \"\"\"\n\u001b[0;32m--\u003e 524\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    525\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    526\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mignore_unicode_prefix\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\n\u001b[0;32m/opt/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1253\u001b[0m             \u001b[0mproto\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEND_COMMAND_PART\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1254\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-\u003e 1255\u001b[0;31m         \u001b[0manswer\u001b[0m \u001b[0;34m\u003d\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1256\u001b[0m         return_value \u003d get_return_value(\n\u001b[1;32m   1257\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\n\u001b[0;32m/opt/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36msend_command\u001b[0;34m(self, command, retry, binary)\u001b[0m\n\u001b[1;32m    983\u001b[0m         \u001b[0mconnection\u001b[0m \u001b[0;34m\u003d\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_connection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    984\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--\u003e 985\u001b[0;31m             \u001b[0mresponse\u001b[0m \u001b[0;34m\u003d\u001b[0m \u001b[0mconnection\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    986\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mbinary\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    987\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_connection_guard\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconnection\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\n\u001b[0;32m/opt/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36msend_command\u001b[0;34m(self, command)\u001b[0m\n\u001b[1;32m   1150\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1151\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-\u003e 1152\u001b[0;31m             \u001b[0manswer\u001b[0m \u001b[0;34m\u003d\u001b[0m \u001b[0msmart_decode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1153\u001b[0m             \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Answer received: {0}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manswer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1154\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0manswer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mproto\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRETURN_MESSAGE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\n\u001b[0;32m/usr/lib64/python3.7/socket.py\u001b[0m in \u001b[0;36mreadinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    587\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    588\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--\u003e 589\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    590\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    591\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_timeout_occurred\u001b[0m \u001b[0;34m\u003d\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\n\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1627392913321_1655927669",
      "id": "20210727-133513_1982076653",
      "dateCreated": "2021-07-27 13:35:13.321",
      "dateStarted": "2021-07-27 14:15:34.021",
      "dateFinished": "2021-07-27 14:15:47.725",
      "status": "ABORT",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%spark.pyspark\n\ntemp \u003d raw_sources_df.limit(5).toPandas()\ntemp.shape",
      "user": "dcr",
      "dateUpdated": "2021-07-27 15:35:33.276",
      "config": {
        "colWidth": 12.0,
        "fontSize": 9.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "python",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "editorMode": "ace/mode/python"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "(5, 19)"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1627297973381_-1776879983",
      "id": "20210726-111253_2071497529",
      "dateCreated": "2021-07-26 11:12:53.381",
      "dateStarted": "2021-07-27 14:18:04.298",
      "dateFinished": "2021-07-27 14:18:04.547",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "title": "Show Tables",
      "text": "%spark.pyspark\n\nspark.catalog.listTables()",
      "user": "dcr",
      "dateUpdated": "2021-07-27 13:27:19.848",
      "config": {
        "colWidth": 12.0,
        "fontSize": 9.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "python",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "editorMode": "ace/mode/python",
        "title": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "[Table(name\u003d\u0027gaia_source\u0027, database\u003d\u0027gaiaedr3\u0027, description\u003dNone, tableType\u003d\u0027EXTERNAL\u0027, isTemporary\u003dFalse),\n Table(name\u003d\u0027gaia_source_allwise_best_neighbours\u0027, database\u003d\u0027gaiaedr3\u0027, description\u003dNone, tableType\u003d\u0027EXTERNAL\u0027, isTemporary\u003dFalse),\n Table(name\u003d\u0027gaia_source_ps1_best_neighbours\u0027, database\u003d\u0027gaiaedr3\u0027, description\u003dNone, tableType\u003d\u0027EXTERNAL\u0027, isTemporary\u003dFalse),\n Table(name\u003d\u0027gaia_source_tmasspsc_best_neighbours\u0027, database\u003d\u0027gaiaedr3\u0027, description\u003dNone, tableType\u003d\u0027EXTERNAL\u0027, isTemporary\u003dFalse),\n Table(name\u003d\u0027dcr_raw_sources\u0027, database\u003dNone, description\u003dNone, tableType\u003d\u0027TEMPORARY\u0027, isTemporary\u003dTrue)]"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1625581158073_12553115",
      "id": "20210706-141918_2051034386",
      "dateCreated": "2021-07-06 14:19:18.073",
      "dateStarted": "2021-07-27 13:27:19.885",
      "dateFinished": "2021-07-27 13:27:20.156",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "title": "Show CAMD Unsorted",
      "text": "%spark.pyspark\n\ndef CAMD(passbands, catalogues, colors \u003d [\"k\", \u0027darkred\u0027, \u0027darkblue\u0027], ms \u003d 1, labels \u003d None, limit \u003d 10000):\n    \"\"\"plot an observational Hertzsprung-Russell diagram (aka colour / absolute magnitude diagram)\n    for the unclassified sample to show the problem,\n    include the photometric consistency filter to show the problem is astrometric in addition to photometric\"\"\"\n    \n    import matplotlib.pylab as plt\n    fig \u003d plt.figure(0, figsize \u003d (9.0, 9.0))\n    c \u003d -1\n\n    for i in catalogues:\n        c+\u003d1\n        if type(ms) \u003d\u003d list:\n            s \u003d ms[c]\n        else: s \u003d ms\n        if type(labels) \u003d\u003d type(None):\n            label \u003d i\n        else: label \u003d labels[c]\n        unclassified_camd_df \u003d spark.sql(\"SELECT phot_{0}_mean_mag + 5.0*LOG10(parallax/100.0) AS m_{0}, {1} FROM {2}\"\\\n                                          .format(passbands[0], passbands[1], i))\n\n        x \u003d unclassified_camd_df.select(\"g_rp\").toPandas()[\"g_rp\"][:limit]\n        y \u003d unclassified_camd_df.select(\"m_g\").toPandas()[\"m_g\"][:limit]\n        plt.scatter(x, y, marker \u003d \u0027.\u0027, s \u003d s, c \u003d colors[c], label \u003d label)\n    plt.ylim(21.0, -3.0)\n    plt.ylabel(\"Stellar brightness (absolute G magnitude) --\u003e\", fontsize \u003d 16)\n    plt.xlabel(\"\u003c-- Stellar temperature (G - RP magnitude)\", fontsize \u003d 16)\n    lgnd \u003d plt.legend(fontsize \u003d 12, markerscale \u003d 1)\n    for i in range(len(catalogues)):\n        lgnd.legendHandles[i]._sizes \u003d [25]\n\n# CAMD(passbands \u003d [\"g\", \"g_rp\"], catalogues \u003d [\"dcr_raw_sources\"], colors \u003d [\"k\"], limit \u003d 100)",
      "user": "dcr",
      "dateUpdated": "2021-07-26 13:22:06.091",
      "config": {
        "colWidth": 12.0,
        "fontSize": 9.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "python",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "editorMode": "ace/mode/python",
        "title": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": []
      },
      "apps": [],
      "jobName": "paragraph_1625584343844_983222154",
      "id": "20210706-151223_1219291343",
      "dateCreated": "2021-07-06 15:12:23.844",
      "dateStarted": "2021-07-26 13:22:41.397",
      "dateFinished": "2021-07-26 13:22:41.625",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "title": "Get HEALPix-6 Factor",
      "text": "%spark.pyspark\n\ndef healpix_level_N(source_id, level, constant \u003d False):\n    \u0027\u0027\u0027returns the HEALpix pixel from Gaia Source ID\u0027\u0027\u0027\n    if constant \u003d\u003d True:\n        return 2**35 * 4**(12-level)\n    else:    \n        return np.floor(source_id/2**35 * 4**(12-level))\n",
      "user": "dcr",
      "dateUpdated": "2021-07-27 12:54:41.703",
      "config": {
        "colWidth": 12.0,
        "fontSize": 9.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "python",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "editorMode": "ace/mode/python",
        "title": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": []
      },
      "apps": [],
      "jobName": "paragraph_1625583464651_751053772",
      "id": "20210706-145744_1307665902",
      "dateCreated": "2021-07-06 14:57:44.652",
      "dateStarted": "2021-07-27 12:54:41.737",
      "dateFinished": "2021-07-27 12:54:41.863",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%md\n\n### Sort *\u0027Labelled\u0027* Training Data\n\n- Good data \u003d each HEALPix level-6 pixel that contains no sources with parallax_over_error \u003c −3.5.\n- Bad data \u003d parallax_over_error \u003c −4.5\n\n### With further split into SNR bins\n\n* High SNR \u003d parallax_over_error (SNR) \u003e 4.5\n* Low SNR \u003d -3.5 \u003c SNR \u003c 4.5",
      "user": "dcr",
      "dateUpdated": "2021-07-27 12:54:43.487",
      "config": {
        "colWidth": 12.0,
        "fontSize": 9.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true,
          "completionKey": "TAB",
          "completionSupport": false
        },
        "editorMode": "ace/mode/markdown",
        "editorHide": true,
        "tableHide": false
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "\u003cdiv class\u003d\"markdown-body\"\u003e\n\u003ch3\u003eSort \u003cem\u003e\u0026lsquo;Labelled\u0026rsquo;\u003c/em\u003e Training Data\u003c/h3\u003e\n\u003cul\u003e\n  \u003cli\u003eGood data \u003d each HEALPix level-6 pixel that contains no sources with parallax_over_error \u0026lt; −3.5.\u003c/li\u003e\n  \u003cli\u003eBad data \u003d parallax_over_error \u0026lt; −4.5\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3\u003eWith further split into SNR bins\u003c/h3\u003e\n\u003cul\u003e\n  \u003cli\u003eHigh SNR \u003d parallax_over_error (SNR) \u0026gt; 4.5\u003c/li\u003e\n  \u003cli\u003eLow SNR \u003d -3.5 \u0026lt; SNR \u0026lt; 4.5\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/div\u003e"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1625583467804_1982882647",
      "id": "20210706-145747_58341513",
      "dateCreated": "2021-07-06 14:57:47.804",
      "dateStarted": "2021-07-27 12:54:43.484",
      "dateFinished": "2021-07-27 12:54:43.492",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "title": "Define features",
      "text": "%spark.pyspark\n\nfactor \u003d healpix_level_N(source_id \u003d None, level \u003d 6, constant \u003d True)\n\ndef getFeatures(withPhotometric \u003d False):\n    # select features to use\n    features \u003d [\n        \u0027source_id\u0027, \u0027parallax_error\u0027, \u0027parallax_over_error\u0027,\n        \u0027pmra\u0027, \u0027astrometric_sigma_5d_max\u0027, \u0027pmdec\u0027,\n        \u0027pmdec_error\u0027, \u0027pmra_error\u0027, \u0027astrometric_excess_noise\u0027,\n        \u0027visibility_periods_used\u0027, \u0027ruwe\u0027, \u0027astrometric_gof_al\u0027,\n        \u0027ipd_gof_harmonic_amplitude\u0027, \u0027ipd_frac_odd_win\u0027, \u0027ipd_frac_multi_peak\u0027,]\n        \n    if withPhotometric:\n        features.extend([\u0027m_g\u0027, \u0027g_rp\u0027])\n    \n    return features\n\nfeatures \u003d getFeatures()",
      "user": "dcr",
      "dateUpdated": "2021-07-27 12:54:46.091",
      "config": {
        "colWidth": 12.0,
        "fontSize": 9.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "python",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "editorMode": "ace/mode/python",
        "title": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": []
      },
      "apps": [],
      "jobName": "paragraph_1625839043312_909039022",
      "id": "20210709-135723_1861639683",
      "dateCreated": "2021-07-09 13:57:23.312",
      "dateStarted": "2021-07-27 12:54:46.120",
      "dateFinished": "2021-07-27 12:54:46.253",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "title": "Select Good or Bad Sources",
      "text": "%spark.pyspark\n\ndef select_bad_sources(table):\n    \u0027\u0027\u0027Selects bad sources from eDR3\u0027\u0027\u0027\n    \n    bad \u003d spark.sql(f\u0027SELECT a.* \\\n                    FROM {table} as a\\\n                    WHERE parallax_over_error \u003c -4.5\u0027)  ### USUALLY -4.5!!!\n    # print(f\u0027{bad.count()} bad sources\u0027)\n    print(f\u0027Collected bad sources\u0027)\n    \n    return bad\n\ndef select_good_sources(table):\n    \u0027\u0027\u0027Selects good sources from eDR3\u0027\u0027\u0027\n    \n    good \u003d spark.sql(f\u0027SELECT a.* \\\n    FROM {table} as a \\\n    WHERE (a.parallax_over_error \u003e 4.5 AND (a.phot_g_mean_mag - a.phot_rp_mean_mag) \u003c1.8) \\\n    OR (a.parallax_over_error \u003c 4.5 AND a.parallax_over_error \u003e -3.0 AND \\\n    (a.phot_g_mean_mag - a.phot_rp_mean_mag) \u003c1.5)\u0027)\n    # print(f\u0027{good.count()} good sources\u0027)\n    print(f\u0027Collected Good sources\u0027)\n    return good\n\ndef with_flags(table, features \u003d \"*\"):\n    \u0027\u0027\u0027Collects data with an \"is_good\" flag for trianing NN\u0027\u0027\u0027\n    \n    # Select training data with flag for good or bad data.\n    good \u003d select_good_sources(table).select(features).withColumn(\u0027is_good\u0027, lit(1))\n    bad \u003d select_bad_sources(table).select(features).withColumn(\u0027is_good\u0027, lit(0))\n    \n    factor \u003d healpix_level_N(source_id \u003d None, level \u003d 6, constant \u003d True)\n    \n    # Join data with a column for |SNR|\n    df \u003d good.union(bad)\\\n                    .withColumn(\u0027abs_SNR\u0027, \n                                f.abs(col(\u0027parallax_over_error\u0027)))\\\n                    .withColumn(\u0027hpx6\u0027, floor(col(\u0027source_id\u0027)/factor))\n    print(\u0027DataBase with flags\u0027)\n    \n    return df",
      "user": "dcr",
      "dateUpdated": "2021-07-27 12:54:47.948",
      "config": {
        "colWidth": 12.0,
        "fontSize": 9.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "python",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "editorMode": "ace/mode/python",
        "title": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": []
      },
      "apps": [],
      "jobName": "paragraph_1625839042580_173330254",
      "id": "20210709-135722_1405384887",
      "dateCreated": "2021-07-09 13:57:22.580",
      "dateStarted": "2021-07-27 12:54:47.974",
      "dateFinished": "2021-07-27 12:54:48.071",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "title": "Drop Bad Pixels",
      "text": "%spark.pyspark\n\ndef drop_bad_pixels(data):\n    \u0027\u0027\u0027drop good sources from HEALpix6 pixels that contain bad sources\u0027\u0027\u0027\n    \n    # Identify bad pixels\n    bad_pixels \u003d set([int(i[0]) for i in df.filter(col(\u0027parallax_over_error\u0027) \u003c -3.5)\\\n                      .select(\u0027hpx6\u0027).toPandas().values])\n    # print(f\u0027Input: {data.filter(col(\"is_good\") \u003d\u003d 1).count()} good sources\u0027)\n    \n    # Drop pixels that contain bad datapoints\n    data \u003d data.filter((col(\u0027is_good\u0027) \u003d\u003d 0) | \n                   ((col(\u0027is_good\u0027) \u003d\u003d 1) \u0026 \n                    (~data.hpx6.isin(bad_pixels))))\n    N_pixels \u003d len(bad_pixels)\n    # print(f\u0027Output: {data.filter(col(\"is_good\") \u003d\u003d 1).count()} good sources from {len(bad_pixels)} pixels\u0027)\n    print(f\u0027dropped bad pixels, {N_pixels}\u0027)\n    return data\n\ndf \u003d with_flags(table \u003d \u0027dcr_raw_sources\u0027, features \u003d features)\n# df \u003d drop_bad_pixels(df)",
      "user": "dcr",
      "dateUpdated": "2021-07-27 12:54:50.299",
      "config": {
        "colWidth": 12.0,
        "fontSize": 9.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "python",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "editorMode": "ace/mode/python",
        "title": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "Collected Good sources\nCollected bad sources\nDataBase with flags\n"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1625839041704_-1227947627",
      "id": "20210709-135721_329827109",
      "dateCreated": "2021-07-09 13:57:21.704",
      "dateStarted": "2021-07-27 12:54:50.324",
      "dateFinished": "2021-07-27 12:54:50.735",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%md\n# Regimes:\n\n### Low |SNR|\n* |SNR| \u003c 4.5 but training data omits data with |SNR| \u003c 4.5 to prevent the imbalance in coverage of SNR-space in the good and bad training sets from impacting our classifications in the low-SNR regime.\n* Does not include |SNR| as a feature.\n\n### High |SNR|\n* Uses the entire training set",
      "user": "dcr",
      "dateUpdated": "2021-07-26 14:52:42.801",
      "config": {
        "colWidth": 12.0,
        "fontSize": 9.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true,
          "completionKey": "TAB",
          "completionSupport": false
        },
        "editorMode": "ace/mode/markdown",
        "title": false,
        "editorHide": true,
        "tableHide": false
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "\u003cdiv class\u003d\"markdown-body\"\u003e\n\u003ch1\u003eRegimes:\u003c/h1\u003e\n\u003ch3\u003eLow |SNR|\u003c/h3\u003e\n\u003cul\u003e\n  \u003cli\u003e|SNR| \u0026lt; 4.5 but training data omits data with |SNR| \u0026lt; 4.5 to prevent the imbalance in coverage of SNR-space in the good and bad training sets from impacting our classifications in the low-SNR regime.\u003c/li\u003e\n  \u003cli\u003eDoes not include |SNR| as a feature.\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3\u003eHigh |SNR|\u003c/h3\u003e\n\u003cul\u003e\n  \u003cli\u003eUses the entire training set\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/div\u003e"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1625839040493_-1840563787",
      "id": "20210709-135720_1210777367",
      "dateCreated": "2021-07-09 13:57:20.493",
      "dateStarted": "2021-07-26 14:52:42.800",
      "dateFinished": "2021-07-26 14:52:42.814",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "title": "Return correct data for regime",
      "text": "%spark.pyspark\n\ndef return_low_snr(df, features \u003d \u0027*\u0027):\n    \u0027\u0027\u0027Creates low SNR training dataset. (-3.5 \u003c SNR \u003c 4.5)\n    This dataset does not include parallax over error as a training feature\n    Counter intuitively, this dataset only includes data with |SNR| \u003e 4.5.\u0027\u0027\u0027\n    df \u003d df.select(features)\\\n                .filter((col(\u0027abs_SNR\u0027) \u003e 4.5) | (col(\u0027is_good\u0027) \u003d\u003d 0))\n    \n#     print(f\u0027 Low SNR dataset contains {df.filter(df.is_good \u003d\u003d 1).count()} \\\n# \"good\" data points and {df.filter(df.is_good \u003d\u003d 0).count()} \"bad\" datapoints.\u0027)\n    print(\u0027Low SNR dataset returned\u0027)\n    return df\n\ndef return_high_snr(df, features \u003d \u0027*\u0027):\n    \u0027\u0027\u0027Creates high SNR training dataset. (|SNR| \u003e 4.5)\n    This dataset does include parallax over error as a training feature.\u0027\u0027\u0027\n    \n#     print(f\u0027High SNR dataset contains {df.filter(df.is_good \u003d\u003d 1).count()} \\\n# \"good\" data points and {df.filter(df.is_good \u003d\u003d 0).count()} \"bad\" datapoints.\u0027)\n    print(\u0027High SNR dataset returned\u0027)\n    \n    return df.select(features)\n",
      "user": "dcr",
      "dateUpdated": "2021-07-27 12:54:58.342",
      "config": {
        "colWidth": 12.0,
        "fontSize": 9.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "python",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "editorMode": "ace/mode/python",
        "title": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": []
      },
      "apps": [],
      "jobName": "paragraph_1625839712240_-357687631",
      "id": "20210709-140832_772076010",
      "dateCreated": "2021-07-09 14:08:32.240",
      "dateStarted": "2021-07-27 12:54:58.371",
      "dateFinished": "2021-07-27 12:54:58.607",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "title": "Split into Train and Test Data",
      "text": "%spark.pyspark\n\ndef split_data(data, split \u003d 0.2, seed \u003d 42):\n    \u0027\u0027\u0027Splits a SQL.DataFrame into independent training and test datasets\u0027\u0027\u0027\n    \n    return data.randomSplit([1-split, split], seed)\n\ndef get_training_data(df, regime, split \u003d 0.2, seed \u003d 42,):\n    \u0027\u0027\u0027returns tuple of DataFrames of training and test data \n       for either High SNR or Low SNR regimes.\u0027\u0027\u0027\n    if \u0027high\u0027 in regime:\n        df \u003d return_high_snr(df)\n        \n    elif \u0027low\u0027 in regime:\n        df \u003d return_low_snr(df)\n        \n    else: raise NameError(\u0027regime is either \"high\" or \"low\"\u0027)\n    return split_data(df, split, seed)",
      "user": "dcr",
      "dateUpdated": "2021-07-27 12:55:00.080",
      "config": {
        "colWidth": 12.0,
        "fontSize": 9.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "python",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "editorMode": "ace/mode/python",
        "title": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": []
      },
      "apps": [],
      "jobName": "paragraph_1625839720004_956296500",
      "id": "20210709-140840_427510120",
      "dateCreated": "2021-07-09 14:08:40.004",
      "dateStarted": "2021-07-27 12:55:00.108",
      "dateFinished": "2021-07-27 12:55:00.234",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "title": "Select Training Features",
      "text": "%spark.pyspark\n\ndef select_training_features(df, regime):\n    \u0027\u0027\u0027selects correct features for the given regime\u0027\u0027\u0027\n    \n    features \u003d [i for i in df.columns if i not in [\u0027source_id\u0027, \u0027parallax_over_error\u0027, \n                                                   \u0027is_good\u0027, \u0027hpx6\u0027, \u0027m_g\u0027, \u0027g_rp\u0027]]\n    \n    if \u0027low\u0027 in regime:\n        features.remove(\u0027abs_SNR\u0027)\n    elif not \u0027high\u0027 in regime: raise NameError(\u0027regime is either \"high\" or \"low\"\u0027)\n    return features",
      "user": "dcr",
      "dateUpdated": "2021-07-27 12:55:01.548",
      "config": {
        "colWidth": 12.0,
        "fontSize": 9.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "python",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "editorMode": "ace/mode/python",
        "title": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": []
      },
      "apps": [],
      "jobName": "paragraph_1625839721950_-37165482",
      "id": "20210709-140841_1557013711",
      "dateCreated": "2021-07-09 14:08:41.950",
      "dateStarted": "2021-07-27 12:55:01.573",
      "dateFinished": "2021-07-27 12:55:01.635",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%md\n\n# Normalization\n",
      "user": "dcr",
      "dateUpdated": "2021-07-27 12:55:03.003",
      "config": {
        "colWidth": 12.0,
        "fontSize": 9.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true,
          "completionKey": "TAB",
          "completionSupport": false
        },
        "editorMode": "ace/mode/markdown",
        "editorHide": true,
        "tableHide": false
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "\u003cdiv class\u003d\"markdown-body\"\u003e\n\u003ch1\u003eNormalization\u003c/h1\u003e\n\u003c/div\u003e"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1625839841769_2030826874",
      "id": "20210709-141041_2000243903",
      "dateCreated": "2021-07-09 14:10:41.769",
      "dateStarted": "2021-07-27 12:55:03.003",
      "dateFinished": "2021-07-27 12:55:03.006",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "title": "getScaler",
      "text": "%spark.pyspark\n\ndef getScaler(scalerType):\n    \u0027\u0027\u0027Returns correct scaler for chosen normalization.\u0027\u0027\u0027\n    \n    if scalerType \u003d\u003d \u0027standard\u0027: \n        from pyspark.ml.feature import StandardScaler\n        Scaler \u003d StandardScaler(withMean\u003dTrue, withStd \u003d True)\n    elif scalerType \u003d\u003d \u0027MinMax\u0027:\n        from pyspark.ml.feature import MinMaxScaler\n        Scaler \u003d MinMaxScaler()\n    else: raise NameError(f\u0027scalerType can be either \"MinMax\" or \"standard\" not {scaler}\u0027)\n    return Scaler\n\n\ndef getScalerModel(df, scalerType \u003d \u0027standard\u0027, featuresCol \u003d \u0027features\u0027,\n                       outCol \u003d \u0027norm_features\u0027, save \u003d False):\n    \u0027\u0027\u0027normalizes DenseVectors for MLlib\u0027\u0027\u0027\n    \n    scaler \u003d getScaler(scalerType \u003d scalerType) # Get scaler\n    scaler.setInputCol(featuresCol)         # Set feature column name\n    model \u003d scaler.fit(df)\n    model.setOutputCol(outCol)\n    if save:\n        model.save(f\u0027{save}/{scalerType}_normalisation_model\u0027)\n        print(f\u0027Normalisation model saved at \"{save}/{scalerType}_normalisation_model\"\u0027)\n    return model\n\ndef normaliseData(df, model,):\n    \u0027\u0027\u0027normalizes featureCol of df using model from \"getScalerModel\" \u0027\u0027\u0027\n\n    return model.transform(df)",
      "user": "dcr",
      "dateUpdated": "2021-07-27 12:55:04.363",
      "config": {
        "colWidth": 12.0,
        "fontSize": 9.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "python",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "editorMode": "ace/mode/python",
        "title": false
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": []
      },
      "apps": [],
      "jobName": "paragraph_1626107486964_960926674",
      "id": "20210712-163126_1792127427",
      "dateCreated": "2021-07-12 16:31:26.964",
      "dateStarted": "2021-07-27 12:55:04.389",
      "dateFinished": "2021-07-27 12:55:04.453",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "title": "Build the Model",
      "text": "%spark.pyspark\n\ndef dataSparkML(df, regime, labelCol \u003d \u0027labels\u0027, featuresCol \u003d \u0027features\u0027, \n                normalise \u003d False, scalerType \u003d \u0027standard\u0027, save \u003d False):\n    \u0027\u0027\u0027get train and test dataset for input using pyspark.ml\u0027\u0027\u0027\n\n    # Get data and label columns for the selected regime\n    data  \u003d get_training_data(df, regime \u003d regime)\n    train_features \u003d select_training_features(data[0], regime \u003d regime)\n    N_features \u003d len(train_features)\n    # Assemble assember to merge features into DenseVector\n    assembler \u003d VectorAssembler(inputCols\u003dtrain_features,\n                                outputCol\u003dfeaturesCol)\n    \n    \n    train \u003d assembler.transform(data[0])\\\n                .select([\u0027source_id\u0027, featuresCol, labelCol])\n    test  \u003d assembler.transform(data[1])\\\n                .select([\u0027source_id\u0027, featuresCol, labelCol])\n    \n    if normalise:\n        # We rename featuresCol to maintain col name for later analysis\n        model \u003d getScalerModel(train.withColumnRenamed(featuresCol, \u0027inputFeatures\u0027),\n                               scalerType \u003d scalerType, featuresCol \u003d \u0027inputFeatures\u0027, \n                               outCol \u003d featuresCol, save \u003d save)\n        \n        # Normalise each dataset with the training data distributions\n        train \u003d normaliseData(train.withColumnRenamed(featuresCol, \u0027inputFeatures\u0027), model)\n        test  \u003d normaliseData( test.withColumnRenamed(featuresCol, \u0027inputFeatures\u0027), model)\n    \n    return (train, test, N_features)\n\n# train,test, N_features \u003d dataSparkML(df, regime \u003d \u0027low\u0027, labelCol \u003d \u0027is_good\u0027)",
      "user": "dcr",
      "dateUpdated": "2021-07-27 12:55:06.702",
      "config": {
        "colWidth": 12.0,
        "fontSize": 9.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "python",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "editorMode": "ace/mode/python",
        "title": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": []
      },
      "apps": [],
      "jobName": "paragraph_1625839841352_1320842855",
      "id": "20210709-141041_25617336",
      "dateCreated": "2021-07-09 14:10:41.352",
      "dateStarted": "2021-07-27 12:55:06.730",
      "dateFinished": "2021-07-27 12:55:06.834",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "title": "MLlib_confusion_matrix",
      "text": "%spark.pyspark\n\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pylab as plt\nfrom collections import Counter\n\nclass MLlib_confusion_matrix():\n    \u0027\u0027\u0027subclass to plot confusion matrix\u0027\u0027\u0027\n    def __init__(self, df, labelCol \u003d \u0027label\u0027, classes \u003d None, normalized \u003d True):\n        self.df               \u003d df\n        self.getLabelCol      \u003d labelCol\n        self.grouped          \u003d df.groupBy(labelCol, \u0027prediction\u0027).count().toPandas()\n        self.matrix           \u003d self.getConfusionMatrix()\n        self.normMatrix       \u003d self.getNormConfusionMatrix()\n#         self.confusion_matrix \u003d self.confusionMatrix(classes, normalized)\n\n    def __repr__(self) : return f\"\"\"Class for plotting confusion matrix {self.normMatrix}\"\"\"\n\n    def getConfusionMatrix(self):\n        \u0027\u0027\u0027returns confusion matrix based on values\u0027\u0027\u0027\n\n        N_classes \u003d len(set(self.grouped[self.getLabelCol]))\n        matrix \u003d np.empty((N_classes, N_classes))\n        for i in range(N_classes):\n            for j in range(N_classes):\n                c \u003d self.grouped[(self.grouped[self.getLabelCol] \u003d\u003d i) \u0026 \n                                 (self.grouped.prediction \u003d\u003d j)][\u0027count\u0027].values\n                if len(c) \u003d\u003d 0:\n                    c \u003d 0 \n                matrix[i][j] \u003d  c\n        return matrix\n\n    def getNormConfusionMatrix(self):\n        \u0027\u0027\u0027returns confusion matrix based on values\u0027\u0027\u0027\n\n        N_classes \u003d len(set(self.grouped[self.getLabelCol]))\n        matrix \u003d np.empty((N_classes, N_classes))\n        for i in range(N_classes):\n            for j in range(N_classes):\n                c \u003d self.grouped[(self.grouped[self.getLabelCol] \u003d\u003d i) \u0026 \n                        (self.grouped.prediction \u003d\u003d j)][\u0027count\u0027].values\\\n                            /sum(self.grouped[(self.grouped[self.getLabelCol] \u003d\u003d i)][\u0027count\u0027])\n                if len(c) \u003d\u003d 0:\n                    c \u003d 0 \n                matrix[i][j] \u003d  np.round(c , 4)\n        return matrix\n    \n    def confusionMatrix(self, classes \u003d None, normalized \u003d True):\n        \u0027\u0027\u0027prints rich version of confusion matrix\u0027\u0027\u0027\n\n        if normalized: matrix \u003d self.normMatrix\n        else: matrix \u003d self.matrix\n        if classes \u003d\u003d None:\n            classes \u003d range(len(matrix))\n\n        N_classes \u003d range(len(classes))\n        plt.rcParams[\u0027figure.figsize\u0027] \u003d (6,6)\n\n        plt.imshow(matrix, cmap \u003d \u0027Greens\u0027, alpha \u003d 0.75)\n        for i in N_classes: # Add values to max pooling\n            for j in N_classes:\n                text \u003d plt.text(j, i, matrix[i][j],\n                               ha\u003d\"center\", va\u003d\"center\", color\u003d\"k\", fontsize \u003d 20)\n\n        #Add thick line to matrix\n        axis \u003d plt.gca()\n        axis.set_yticks(np.arange(-0.5, len(classes)-0.5, 1), minor\u003d\u0027True\u0027)\n        axis.set_xticks(np.arange(-0.5, len(classes)-0.5, 1), minor\u003d\u0027True\u0027)\n        axis.yaxis.grid(True, which\u003d\u0027minor\u0027, color \u003d \u0027k\u0027, lw \u003d 2)\n        axis.xaxis.grid(True, which\u003d\u0027minor\u0027, color \u003d \u0027k\u0027, lw \u003d 2)\n        plt.xticks(N_classes, classes, rotation \u003d0, fontsize \u003d 14)\n        plt.yticks(N_classes, classes, rotation \u003d0, fontsize \u003d 14)\n        plt.xlabel(\u0027Predicted Class\u0027, fontsize \u003d 16)\n        plt.ylabel(\u0027True Class\u0027, fontsize \u003d 16)\n        \n        \nclass MLlibMultiClassEvaluator(MLlib_confusion_matrix):\n    \u0027\u0027\u0027class to calculate parameters of NN performance for binary classification\u0027\u0027\u0027\n    \n    def __init__(self, df, labelCol \u003d \u0027label\u0027):\n        self.df            \u003d df\n        self.getLabelCol   \u003d labelCol\n        self.show          \u003d df.show\n        self.count         \u003d df.count\n        self.shape         \u003d (df.count(), len(df.columns))\n        self.grouped       \u003d df.groupBy(labelCol, \u0027prediction\u0027).count().toPandas()\n        self.matrix        \u003d self.getConfusionMatrix()\n        \n    def __len__(self)         : return self.df.count()\n    def __repr__(self)        : return f\"\"\"Evaluate ML performance for {len(self)} datapoints.\"\"\"\n    \n    def getAnalysis(self):\n        \u0027\u0027\u0027returns a pd.DataFrame for analysis\u0027\u0027\u0027\n        data \u003d self.df.select(self.getLabelCol,\u0027probability\u0027, \u0027prediction\u0027).toPandas()\n        for i in range(max(data[self.getLabelCol])+1):\n            data[f\u0027prob\u0027] \u003d [max(j) for j in data.probability]\n        return data\n    \n    def getTruePositives(self, step \u003d np.arange(0.0, 0.95, 0.01)):\n        \u0027\u0027\u0027calculates how True Positives change by threshold\u0027\u0027\u0027\n        conMat \u003d self.getConfusionMatrix()\n        data \u003d self.getAnalysis()\n        tp \u003d {}\n        for cat in range(len(conMat)):\n            tp[cat] \u003d []\n        for cat in range(len(conMat)):\n            for threshold in step:\n                tmp \u003d Counter(data[(data[\u0027prediction\u0027] \u003d\u003d data[self.getLabelCol]) \u0026 \n                                   (data[f\u0027prob\u0027]\u003ethreshold)][self.getLabelCol])\n                for cat in range(len(conMat)):\n                    tp[cat].append(tmp[cat])\n            return tp\n        \n        \n    def getFalsePositives(self, step \u003d np.arange(0.0, 0.95, 0.01)):\n        \u0027\u0027\u0027calculates how True Positives change by threshold\u0027\u0027\u0027\n        conMat \u003d self.getConfusionMatrix()\n        data \u003d self.getAnalysis()\n        fp \u003d {}\n        for cat in range(len(conMat)):\n            fp[cat] \u003d []\n        for cat in range(len(conMat)):\n            for threshold in step:\n                tmp \u003d Counter(data[(data[\u0027prediction\u0027] !\u003d data[self.getLabelCol]) \u0026 \n                                   (data[f\u0027prob\u0027]\u003ethreshold)].prediction)\n                for cat in range(len(conMat)):\n                    fp[cat].append(tmp[cat])\n            return fp        \n        \n    def getFalseNegatives(self, step \u003d np.arange(0.0, 0.95, 0.01)):\n        \u0027\u0027\u0027calculates how False Positives change by threshold\u0027\u0027\u0027\n        conMat \u003d self.getConfusionMatrix()\n        data \u003d self.getAnalysis()\n        fn \u003d {}\n        for cat in range(len(conMat)):\n            fn[cat] \u003d []\n        for cat in range(len(conMat)):\n            for threshold in step:\n                tmp \u003d Counter(data[(data[\u0027prediction\u0027] !\u003d data[self.getLabelCol]) \u0026 \n                                   (data[f\u0027prob\u0027]\u003ethreshold)][self.getLabelCol])\n                for cat in range(len(conMat)):\n                    fn[cat].append(tmp[cat])\n            return fn\n        \n    def getPrecision(self, step \u003d np.arange(0.0, 0.95, 0.01)):\n        \u0027\u0027\u0027calculate precision\u0027\u0027\u0027\n        precision \u003d {}; true_positives \u003d self.getTruePositives(step); \n        false_positives \u003d self.getFalsePositives(step)\n        conMat \u003d self.getConfusionMatrix()\n        precision \u003d {}\n        for cat in range(len(conMat)):\n            precision[cat] \u003d conMat[cat][cat]\n            for cat in range(len(conMat)):\n                precision[cat] \u003d [tp / (tp+fp) for tp, fp in zip(true_positives[cat],false_positives[cat])]\n        return precision\n    \n    def getRecall(self, step \u003d np.arange(0.0, 0.95, 0.01)):\n        \u0027\u0027\u0027calculate recall\u0027\u0027\u0027\n        conMat \u003d self.getConfusionMatrix()\n        recall \u003d {}; true_positives \u003d self.getTruePositives(step); \n        false_negatives \u003d self.getFalseNegatives(step)\n        for cat in range(len(conMat)):\n            recall[cat] \u003d []\n            for cat in range(len(conMat)):\n                recall[cat] \u003d [tp / (tp+fn) for tp, fn in zip(true_positives[cat],false_negatives[cat])]\n        return recall\n    \n    \nclass plottingThreshold(MLlibMultiClassEvaluator):\n    \u0027\u0027\u0027subclass of \"threshold\" for various plots of useful threshold parameters.\u0027\u0027\u0027\n    \n    def __repr__(self): return f\"\"\"Plotting software for \u0027threshold\u0027 objects\"\"\"\n    def __init__(self, df, labelCol \u003d \u0027label\u0027):\n        self.df            \u003d df\n        self.getLabelCol   \u003d labelCol\n        self.show          \u003d df.show\n        self.count         \u003d df.count\n        self.shape         \u003d (df.count(), len(df.columns))\n        self.grouped       \u003d df.groupBy(labelCol, \u0027prediction\u0027).count().toPandas()\n        self.matrix        \u003d self.getConfusionMatrix()\n        \n        \n    def getAxes(self,ax):\n        if ax \u003d\u003d None:\n            ax \u003d plt.subplot(111)\n        return ax\n\n    def plot_true_positives(self, ax \u003d None, step \u003d np.arange(0.0, 0.95, 0.01), normalized \u003d True, legend \u003d True):\n        ax \u003d self.getAxes(ax); true_positives \u003d self.getTruePositives(step); \n        conMat \u003d self.getConfusionMatrix()\n        for cat in range(len(conMat)):\n            if normalized \u003d\u003d True:\n                y \u003d [i/max(true_positives[cat]) for i in true_positives[cat]]\n                ax.set_ylabel(\u0027Fraction of Total True Positives --\u003e\u0027)\n            else:\n                y \u003d [i/len(self.data[self.data.true_label \u003d\u003d cat]) for i in true_positives[cat]]\n                ax.set_ylabel(\u0027True Positives --\u003e\u0027)\n            ax.plot(step, y, label \u003d cat)\n        if legend:\n            ax.legend()\n        ax.grid(\u0027on\u0027)\n        ax.set_xlabel(\u0027Threshold\u0027)\n\n    def plot_false_positives(self, ax \u003d None, step \u003d np.arange(0.0, 0.95, 0.01), legend \u003d True):\n        ax \u003d self.getAxes(ax); false_positives \u003d self.getFalsePositives(step)\n        conMat \u003d self.getConfusionMatrix()\n        for cat in range(len(conMat)):\n            if max(false_positives[cat]) !\u003d 0:\n                y \u003d [i/max(false_positives[cat]) for i in false_positives[cat]]\n            else: y \u003d [i for i in false_positives[cat]]\n            ax.plot(step, y, label \u003d cat)\n        if legend:\n            ax.legend()\n        ax.grid(\u0027on\u0027)\n        ax.set_xlabel(\u0027Threshold\u0027)\n        ax.set_ylabel(\u0027\u003c-- Fraction of Total False Positives\u0027)\n\n    def plot_precision(self, ax \u003d None, step \u003d np.arange(0.0, 0.95, 0.01), legend \u003d True):\n        ax \u003d self.getAxes(ax); precision \u003d self.getPrecision(step)\n        conMat \u003d self.getConfusionMatrix()\n        for cat in range(len(conMat)):\n            y \u003d precision[cat]\n            ax.plot(step, y, label \u003d cat)\n        if legend:\n            ax.legend()\n        ax.grid(\u0027on\u0027)\n        ax.set_xlabel(\u0027Threshold\u0027)\n        ax.set_ylabel(\u0027Precision --\u003e\u0027)\n\n    def plot_recall(self, ax \u003d None, step \u003d np.arange(0.0, 0.95, 0.01), legend \u003d True):\n        ax \u003d self.getAxes(ax); recall \u003d self.getRecall(step)\n        conMat \u003d self.getConfusionMatrix()\n        for cat in range(len(conMat)):\n            y \u003d recall[cat]\n            ax.plot(step, y, label \u003d cat)\n        if legend:\n            ax.legend()\n        ax.grid(\u0027on\u0027)\n        ax.set_xlabel(\u0027Threshold\u0027)\n        ax.set_ylabel(\u0027Recall --\u003e\u0027)\n        \n\n    def threshold_subplots(self, step \u003d np.arange(0.0, 0.95, 0.01), figsize\u003d(15, 8)):\n        import matplotlib.gridspec as gridspec\n        fig \u003d plt.figure(figsize \u003d figsize)\n        gs \u003d gridspec.GridSpec(ncols\u003d2, nrows\u003d2, figure\u003dfig)\n        ax1 \u003d fig.add_subplot(gs[0, 0])\n        ax2 \u003d fig.add_subplot(gs[0, 1])\n        ax3 \u003d fig.add_subplot(gs[1, 0])\n        ax4 \u003d fig.add_subplot(gs[1, 1])\n\n        axs \u003d [ax1, ax2, ax3, ax4]\n        self.plot_precision      (ax \u003d axs[0], step \u003d step, legend \u003d False)\n        self.plot_recall         (ax \u003d axs[1], step \u003d step, legend \u003d False)\n        self.plot_true_positives (ax \u003d axs[2], step \u003d step, legend \u003d True )\n        self.plot_false_positives(ax \u003d axs[3], step \u003d step, legend \u003d True )\n        plt.tight_layout()",
      "user": "dcr",
      "dateUpdated": "2021-07-27 12:55:08.822",
      "config": {
        "colWidth": 12.0,
        "fontSize": 9.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "python",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "editorMode": "ace/mode/python",
        "title": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": []
      },
      "apps": [],
      "jobName": "paragraph_1625583466178_2136528193",
      "id": "20210706-145746_766891138",
      "dateCreated": "2021-07-06 14:57:46.178",
      "dateStarted": "2021-07-27 12:55:08.849",
      "dateFinished": "2021-07-27 12:55:08.987",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "title": "Train the model",
      "text": "%spark.pyspark\n\ndef nnTrainModel(df, regime, labelCol \u003d \u0027label\u0027,\n                 featuresCol \u003d \u0027features\u0027,\n                 scalerType \u003d \u0027standard\u0027, save \u003d False, \n                 normalise \u003d False):\n    \u0027\u0027\u0027trains a neural network using sparkML\u0027\u0027\u0027\n    \n    # get train, test and N_features \n    train, test, \\\n        N_features \u003d dataSparkML(df \u003d df, regime \u003d regime, labelCol \u003d labelCol, \n                                 normalise \u003d normalise, featuresCol \u003d featuresCol,\n                                 save \u003d save, scalerType \u003d scalerType)\n    \n    # specify layers for the neural network:\n    layers \u003d [N_features, 64, 64, 64, 64, 2]\n\n    # create the trainer and set its parameters\n    trainer \u003d MultilayerPerceptronClassifier(maxIter\u003d100, layers\u003dlayers, blockSize\u003d128, seed\u003d42)\\\n                    .setLabelCol(labelCol)\n    \n    print(trainer)\n\n    # train the model\n    model \u003d trainer.fit(train)\n    if save:\n        model.save(f\u0027{save}/MultilayerPerceptronClassifier/\u0027)\n    return train, test, model\n\n\ntrain, test, model \u003d nnTrainModel(df \u003d df, regime \u003d \u0027low\u0027, normalise \u003d True,\n                                  labelCol \u003d \u0027is_good\u0027, scalerType \u003d \u0027standard\u0027, \n                                  save \u003d \u0027/user/dcr/ML_cuts/lowSNR/\u0027)",
      "user": "dcr",
      "dateUpdated": "2021-07-27 12:55:15.431",
      "config": {
        "colWidth": 12.0,
        "fontSize": 9.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "python",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "editorMode": "ace/mode/python",
        "title": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "ERROR",
        "msg": [
          {
            "type": "TEXT",
            "data": "Low SNR dataset returned\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)\n\u001b[0;32m\u003cipython-input-40-03c371ab7483\u003e\u001b[0m in \u001b[0;36m\u003cmodule\u003e\u001b[0;34m\u001b[0m\n\u001b[1;32m     29\u001b[0m train, test, model \u003d nnTrainModel(df \u003d df, regime \u003d \u0027low\u0027, normalise \u003d True,\n\u001b[1;32m     30\u001b[0m                                   \u001b[0mlabelCol\u001b[0m \u001b[0;34m\u003d\u001b[0m \u001b[0;34m\u0027is_good\u0027\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscalerType\u001b[0m \u001b[0;34m\u003d\u001b[0m \u001b[0;34m\u0027standard\u0027\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---\u003e 31\u001b[0;31m                                   save \u003d \u0027/user/dcr/ML_cuts/lowSNR/\u0027)\n\u001b[0m\n\u001b[0;32m\u003cipython-input-40-03c371ab7483\u003e\u001b[0m in \u001b[0;36mnnTrainModel\u001b[0;34m(df, regime, labelCol, featuresCol, scalerType, save, normalise)\u001b[0m\n\u001b[1;32m      9\u001b[0m         N_features \u003d dataSparkML(df \u003d df, regime \u003d regime, labelCol \u003d labelCol, \n\u001b[1;32m     10\u001b[0m                                  \u001b[0mnormalise\u001b[0m \u001b[0;34m\u003d\u001b[0m \u001b[0mnormalise\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeaturesCol\u001b[0m \u001b[0;34m\u003d\u001b[0m \u001b[0mfeaturesCol\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---\u003e 11\u001b[0;31m                                  save \u003d save, scalerType \u003d scalerType)\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0;31m# specify layers for the neural network:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\n\u001b[0;32m\u003cipython-input-36-76e678aa5669\u003e\u001b[0m in \u001b[0;36mdataSparkML\u001b[0;34m(df, regime, labelCol, featuresCol, normalise, scalerType, save)\u001b[0m\n\u001b[1;32m     21\u001b[0m         model \u003d getScalerModel(train.withColumnRenamed(featuresCol, \u0027inputFeatures\u0027),\n\u001b[1;32m     22\u001b[0m                                \u001b[0mscalerType\u001b[0m \u001b[0;34m\u003d\u001b[0m \u001b[0mscalerType\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeaturesCol\u001b[0m \u001b[0;34m\u003d\u001b[0m \u001b[0;34m\u0027inputFeatures\u0027\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---\u003e 23\u001b[0;31m                                outCol \u003d featuresCol, save \u003d save)\n\u001b[0m\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0;31m# Normalise each dataset with the training data distributions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\n\u001b[0;32m\u003cipython-input-34-1f1bdb9e4c99\u003e\u001b[0m in \u001b[0;36mgetScalerModel\u001b[0;34m(df, scalerType, featuresCol, outCol, save)\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0mscaler\u001b[0m \u001b[0;34m\u003d\u001b[0m \u001b[0mgetScaler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscalerType\u001b[0m \u001b[0;34m\u003d\u001b[0m \u001b[0mscalerType\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# Get scaler\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0mscaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetInputCol\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeaturesCol\u001b[0m\u001b[0;34m)\u001b[0m         \u001b[0;31m# Set feature column name\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---\u003e 20\u001b[0;31m     \u001b[0mmodel\u001b[0m \u001b[0;34m\u003d\u001b[0m \u001b[0mscaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetOutputCol\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutCol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0msave\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\n\u001b[0;32m/opt/spark/python/lib/pyspark.zip/pyspark/ml/base.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, dataset, params)\u001b[0m\n\u001b[1;32m    130\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    131\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--\u003e 132\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    133\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m             raise ValueError(\"Params must be either a param map or a list/tuple of param maps, \"\n\n\u001b[0;32m/opt/spark/python/lib/pyspark.zip/pyspark/ml/wrapper.py\u001b[0m in \u001b[0;36m_fit\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    293\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    294\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--\u003e 295\u001b[0;31m         \u001b[0mjava_model\u001b[0m \u001b[0;34m\u003d\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit_java\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    296\u001b[0m         \u001b[0mmodel\u001b[0m \u001b[0;34m\u003d\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjava_model\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    297\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_copyValues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\n\u001b[0;32m/opt/spark/python/lib/pyspark.zip/pyspark/ml/wrapper.py\u001b[0m in \u001b[0;36m_fit_java\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    290\u001b[0m         \"\"\"\n\u001b[1;32m    291\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_transfer_params_to_java\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--\u003e 292\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_java_obj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    293\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    294\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\n\u001b[0;32m/opt/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1253\u001b[0m             \u001b[0mproto\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEND_COMMAND_PART\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1254\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-\u003e 1255\u001b[0;31m         \u001b[0manswer\u001b[0m \u001b[0;34m\u003d\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1256\u001b[0m         return_value \u003d get_return_value(\n\u001b[1;32m   1257\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\n\u001b[0;32m/opt/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36msend_command\u001b[0;34m(self, command, retry, binary)\u001b[0m\n\u001b[1;32m    983\u001b[0m         \u001b[0mconnection\u001b[0m \u001b[0;34m\u003d\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_connection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    984\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--\u003e 985\u001b[0;31m             \u001b[0mresponse\u001b[0m \u001b[0;34m\u003d\u001b[0m \u001b[0mconnection\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    986\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mbinary\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    987\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_connection_guard\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconnection\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\n\u001b[0;32m/opt/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36msend_command\u001b[0;34m(self, command)\u001b[0m\n\u001b[1;32m   1150\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1151\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-\u003e 1152\u001b[0;31m             \u001b[0manswer\u001b[0m \u001b[0;34m\u003d\u001b[0m \u001b[0msmart_decode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1153\u001b[0m             \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Answer received: {0}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manswer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1154\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0manswer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mproto\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRETURN_MESSAGE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\n\u001b[0;32m/usr/lib64/python3.7/socket.py\u001b[0m in \u001b[0;36mreadinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    587\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    588\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--\u003e 589\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    590\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    591\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_timeout_occurred\u001b[0m \u001b[0;34m\u003d\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\n\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1625839840935_1063460131",
      "id": "20210709-141040_1258934891",
      "dateCreated": "2021-07-09 14:10:40.935",
      "dateStarted": "2021-07-27 12:55:15.460",
      "dateFinished": "2021-07-27 13:19:45.984",
      "status": "ABORT",
      "progressUpdateIntervalMs": 500
    },
    {
      "title": "Evaluate Model",
      "text": "%spark.pyspark\n\ndef evaluateModel(model, df, labelCol \u003d \u0027label\u0027, evaluate \u003d False,\n                  normalise \u003d True,\n                  step \u003d np.arange(0.5, 0.99, 0.01)):\n    \u0027\u0027\u0027Apply a NN to new data, with the option to evaluate if test dataset\u0027\u0027\u0027\n    \n    # compute accuracy on the test set\n    result \u003d model.transform(df)\n    predictionAndLabels \u003d result.select(\"prediction\", \u0027is_good\u0027)\n    evaluator \u003d MulticlassClassificationEvaluator(metricName\u003d\"accuracy\").setLabelCol(labelCol)\n#     print(\"Test set accuracy \u003d \" + str(evaluator.evaluate(predictionAndLabels)))\n    \n    if evaluate:\n        # Call MLlib_confusion_matrix class to plot confusion matrix\n        res \u003d MLlib_confusion_matrix(result, labelCol \u003d labelCol)\n        res.confusionMatrix(classes \u003d [\u0027Bad\u0027, \u0027Good\u0027], normalized \u003d True)\n        \n        # Call plottingThreshold to explore the effect of Thresholding\n        plottingThreshold(result, labelCol \u003d labelCol)\\\n            .threshold_subplots(step \u003d step)\n\n    return result\n\nresult \u003d evaluateModel(model \u003d model, df \u003d test, normalise \u003d True,  evaluate \u003d True, \n                       labelCol \u003d \u0027is_good\u0027)\n\n",
      "user": "dcr",
      "dateUpdated": "2021-07-26 14:54:05.369",
      "config": {
        "colWidth": 12.0,
        "fontSize": 9.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "python",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "editorMode": "ace/mode/python",
        "title": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "ERROR",
        "msg": [
          {
            "type": "TEXT",
            "data": "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)\n\u001b[0;32m\u003cipython-input-44-e18b8faa2373\u003e\u001b[0m in \u001b[0;36m\u003cmodule\u003e\u001b[0;34m\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---\u003e 23\u001b[0;31m result \u003d evaluateModel(model \u003d model, df \u003d test, normalise \u003d True,  evaluate \u003d True, \n\u001b[0m\u001b[1;32m     24\u001b[0m                        labelCol \u003d \u0027is_good\u0027)\n\n\u001b[0;31mNameError\u001b[0m: name \u0027model\u0027 is not defined"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1626107603462_339089471",
      "id": "20210712-163323_837319583",
      "dateCreated": "2021-07-12 16:33:23.462",
      "dateStarted": "2021-07-26 14:55:05.064",
      "dateFinished": "2021-07-26 14:55:05.237",
      "status": "ERROR",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%spark.pyspark\n\ncreate_SQL_queryable(result, \u0027ML_res\u0027)",
      "user": "dcr",
      "dateUpdated": "2021-07-09 19:46:58.274",
      "config": {
        "colWidth": 12.0,
        "fontSize": 9.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "python",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "editorMode": "ace/mode/python"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "ERROR",
        "msg": [
          {
            "type": "TEXT",
            "data": "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)\n\u001b[0;32m\u003cipython-input-34-3e615c5e324b\u003e\u001b[0m in \u001b[0;36m\u003cmodule\u003e\u001b[0;34m\u001b[0m\n\u001b[0;32m----\u003e 1\u001b[0;31m \u001b[0mcreate_SQL_queryable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\u0027ML_res\u0027\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\n\u001b[0;31mNameError\u001b[0m: name \u0027create_SQL_queryable\u0027 is not defined"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1625840791896_-509318819",
      "id": "20210709-142631_1238119948",
      "dateCreated": "2021-07-09 14:26:31.896",
      "dateStarted": "2021-07-09 20:03:14.675",
      "dateFinished": "2021-07-09 20:03:14.722",
      "status": "ERROR",
      "progressUpdateIntervalMs": 500
    },
    {
      "title": "CAMD_2",
      "text": "%spark.pyspark\ndef CAMD(passbands, catalogue, classes, colors \u003d [\u0027k\u0027, \u0027darkred\u0027, \u0027darkblue\u0027], \n         ms \u003d 1, labels \u003d None, limit \u003d 10000):\n    \u0027\u0027\u0027Plot an observational Hertzsprung-Russell diagram (aka colour / absolute magnitude diagram)\n    for the unclassified sample to show the problem,\n    include the photometric consistency filter to show the problem is astrometric in addition to photometric \u0027\u0027\u0027\n    \n    fig \u003d plt.figure(0, figsize \u003d (9.0, 9.0))\n    c \u003d -1\n    for i in classes:\n        c+\u003d1\n        if type(ms) \u003d\u003d list:\n            s \u003d ms[c]\n        else: s \u003d ms\n        if type(labels) \u003d\u003d type(None):\n            label \u003d i\n        else: label \u003d labels[c]\n        unclassified_camd_df \u003d spark.sql(f\u0027SELECT b.prediction , a.source_id,\\\n                                         a.phot_{passbands[0]}_mean_mag + 5.0*LOG10(a.parallax/100.0) \\\n                                         AS m_{passbands[0]}, {passbands[1]} FROM {catalogue} as a, \\\n                                         {\"ML_res\"} as b \\\n                                         WHERE a.source_id \u003d b.source_id\u0027)\n        \n        unclassified_camd_df \u003d unclassified_camd_df.filter(col(\u0027prediction\u0027) \u003d\u003d c)\n        x \u003d unclassified_camd_df.select(\u0027g_rp\u0027).toPandas()[\u0027g_rp\u0027][:limit]\n        y \u003d unclassified_camd_df.select(\u0027m_g\u0027).toPandas()[\u0027m_g\u0027][:limit]\n        plt.scatter(x, y, marker \u003d \u0027.\u0027, s \u003d ms, label \u003d label, color \u003d colors[c])\n    plt.ylim(21.0, -3.0)\n    plt.xlim(0, 3.5)\n    plt.ylabel(\u0027Stellar brightness (absolute G magnitude) --\u003e\u0027, fontsize \u003d 16)\n    plt.xlabel(\u0027\u003c-- Stellar temperature (G - RP magnitude)\u0027, fontsize \u003d 16)\n    lgnd \u003d plt.legend(fontsize \u003d 12)\n    for i in range(len(classes)):\n        lgnd.legendHandles[i]._sizes \u003d [55]\n\nCAMD(passbands \u003d [\u0027g\u0027, \u0027g_rp\u0027], catalogue \u003d \u0027edr3_sources\u0027, \n     colors \u003d [\u0027grey\u0027, \u0027green\u0027], classes \u003d [\u0027Bad\u0027,\u0027Good\u0027], ms \u003d 1, limit \u003d 1000)",
      "user": "dcr",
      "dateUpdated": "2021-07-19 13:50:49.915",
      "config": {
        "colWidth": 12.0,
        "fontSize": 9.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "python",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "editorMode": "ace/mode/python",
        "title": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "ERROR",
        "msg": [
          {
            "type": "TEXT",
            "data": "\u003cFigure size 648x648 with 0 Axes\u003e\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)\n\u001b[0;32m/opt/spark/python/lib/pyspark.zip/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---\u003e 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\n\u001b[0;32m/opt/spark/python/lib/py4j-0.10.7-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--\u003e 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\n\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o67.sql.\n: org.apache.spark.sql.AnalysisException: Table or view not found: edr3_sources; line 1 pos 182\n\tat org.apache.spark.sql.catalyst.analysis.package$AnalysisErrorAt.failAnalysis(package.scala:47)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.org$apache$spark$sql$catalyst$analysis$Analyzer$ResolveRelations$$lookupTableFromCatalog(Analyzer.scala:798)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.resolveRelation(Analyzer.scala:750)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$$anonfun$apply$8.applyOrElse(Analyzer.scala:780)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$$anonfun$apply$8.applyOrElse(Analyzer.scala:773)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveOperatorsUp$1$$anonfun$apply$1.apply(AnalysisHelper.scala:90)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveOperatorsUp$1$$anonfun$apply$1.apply(AnalysisHelper.scala:90)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:69)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveOperatorsUp$1.apply(AnalysisHelper.scala:89)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveOperatorsUp$1.apply(AnalysisHelper.scala:86)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:194)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$class.resolveOperatorsUp(AnalysisHelper.scala:86)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUp(LogicalPlan.scala:29)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveOperatorsUp$1$$anonfun$1.apply(AnalysisHelper.scala:87)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveOperatorsUp$1$$anonfun$1.apply(AnalysisHelper.scala:87)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$4.apply(TreeNode.scala:328)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:186)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:326)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveOperatorsUp$1.apply(AnalysisHelper.scala:87)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveOperatorsUp$1.apply(AnalysisHelper.scala:86)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:194)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$class.resolveOperatorsUp(AnalysisHelper.scala:86)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUp(LogicalPlan.scala:29)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveOperatorsUp$1$$anonfun$1.apply(AnalysisHelper.scala:87)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveOperatorsUp$1$$anonfun$1.apply(AnalysisHelper.scala:87)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$4.apply(TreeNode.scala:328)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:186)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:326)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveOperatorsUp$1.apply(AnalysisHelper.scala:87)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveOperatorsUp$1.apply(AnalysisHelper.scala:86)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:194)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$class.resolveOperatorsUp(AnalysisHelper.scala:86)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUp(LogicalPlan.scala:29)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveOperatorsUp$1$$anonfun$1.apply(AnalysisHelper.scala:87)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveOperatorsUp$1$$anonfun$1.apply(AnalysisHelper.scala:87)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$4.apply(TreeNode.scala:328)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:186)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:326)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveOperatorsUp$1.apply(AnalysisHelper.scala:87)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveOperatorsUp$1.apply(AnalysisHelper.scala:86)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:194)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$class.resolveOperatorsUp(AnalysisHelper.scala:86)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUp(LogicalPlan.scala:29)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveOperatorsUp$1$$anonfun$1.apply(AnalysisHelper.scala:87)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveOperatorsUp$1$$anonfun$1.apply(AnalysisHelper.scala:87)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$4.apply(TreeNode.scala:328)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:186)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:326)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveOperatorsUp$1.apply(AnalysisHelper.scala:87)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveOperatorsUp$1.apply(AnalysisHelper.scala:86)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:194)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$class.resolveOperatorsUp(AnalysisHelper.scala:86)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUp(LogicalPlan.scala:29)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.apply(Analyzer.scala:773)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.apply(Analyzer.scala:719)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1$$anonfun$apply$1.apply(RuleExecutor.scala:87)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1$$anonfun$apply$1.apply(RuleExecutor.scala:84)\n\tat scala.collection.LinearSeqOptimized$class.foldLeft(LinearSeqOptimized.scala:124)\n\tat scala.collection.immutable.List.foldLeft(List.scala:84)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1.apply(RuleExecutor.scala:84)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1.apply(RuleExecutor.scala:76)\n\tat scala.collection.immutable.List.foreach(List.scala:392)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:76)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.org$apache$spark$sql$catalyst$analysis$Analyzer$$executeSameContext(Analyzer.scala:127)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:121)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$$anonfun$executeAndCheck$1.apply(Analyzer.scala:106)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$$anonfun$executeAndCheck$1.apply(Analyzer.scala:105)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:201)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:105)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:58)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:56)\n\tat org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:48)\n\tat org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:78)\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:643)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: org.apache.spark.sql.catalyst.analysis.NoSuchTableException: Table or view \u0027edr3_sources\u0027 not found in database \u0027default\u0027;\n\tat org.apache.spark.sql.catalyst.catalog.ExternalCatalog$class.requireTableExists(ExternalCatalog.scala:48)\n\tat org.apache.spark.sql.catalyst.catalog.InMemoryCatalog.requireTableExists(InMemoryCatalog.scala:45)\n\tat org.apache.spark.sql.catalyst.catalog.InMemoryCatalog.getTable(InMemoryCatalog.scala:326)\n\tat org.apache.spark.sql.catalyst.catalog.ExternalCatalogWithListener.getTable(ExternalCatalogWithListener.scala:138)\n\tat org.apache.spark.sql.catalyst.catalog.SessionCatalog.lookupRelation(SessionCatalog.scala:706)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.org$apache$spark$sql$catalyst$analysis$Analyzer$ResolveRelations$$lookupTableFromCatalog(Analyzer.scala:795)\n\t... 83 more\n\n\nDuring handling of the above exception, another exception occurred:\n\n\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)\n\u001b[0;32m\u003cipython-input-36-90bcb72b32e0\u003e\u001b[0m in \u001b[0;36m\u003cmodule\u003e\u001b[0;34m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m CAMD(passbands \u003d [\u0027g\u0027, \u0027g_rp\u0027], catalogue \u003d \u0027edr3_sources\u0027, \n\u001b[0;32m---\u003e 36\u001b[0;31m      colors \u003d [\u0027grey\u0027, \u0027green\u0027], classes \u003d [\u0027Bad\u0027,\u0027Good\u0027], ms \u003d 1)\n\u001b[0m\n\u001b[0;32m\u003cipython-input-36-90bcb72b32e0\u003e\u001b[0m in \u001b[0;36mCAMD\u001b[0;34m(passbands, catalogue, classes, colors, ms, labels)\u001b[0m\n\u001b[1;32m     19\u001b[0m                                          \u001b[0mAS\u001b[0m \u001b[0mm_\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mpassbands\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mpassbands\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m}\u001b[0m \u001b[0mFROM\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mcatalogue\u001b[0m\u001b[0;34m}\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m                                          \u001b[0;34m{\u001b[0m\u001b[0;34m\"ML_res\"\u001b[0m\u001b[0;34m}\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mb\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---\u003e 21\u001b[0;31m                                          WHERE a.source_id \u003d b.source_id\u0027)\n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0munclassified_camd_df\u001b[0m \u001b[0;34m\u003d\u001b[0m \u001b[0munclassified_camd_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfilter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u0027prediction\u0027\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m\u003d\u003d\u001b[0m \u001b[0mc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\n\u001b[0;32m/opt/spark/python/lib/pyspark.zip/pyspark/sql/session.py\u001b[0m in \u001b[0;36msql\u001b[0;34m(self, sqlQuery)\u001b[0m\n\u001b[1;32m    765\u001b[0m         \u001b[0;34m[\u001b[0m\u001b[0mRow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf1\u001b[0m\u001b[0;34m\u003d\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf2\u001b[0m\u001b[0;34m\u003d\u001b[0m\u001b[0;34mu\u0027row1\u0027\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf1\u001b[0m\u001b[0;34m\u003d\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf2\u001b[0m\u001b[0;34m\u003d\u001b[0m\u001b[0;34mu\u0027row2\u0027\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf1\u001b[0m\u001b[0;34m\u003d\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf2\u001b[0m\u001b[0;34m\u003d\u001b[0m\u001b[0;34mu\u0027row3\u0027\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    766\u001b[0m         \"\"\"\n\u001b[0;32m--\u003e 767\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jsparkSession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msqlQuery\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_wrapped\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    768\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    769\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0msince\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\n\u001b[0;32m/opt/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1255\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m\u003d\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1256\u001b[0m         return_value \u003d get_return_value(\n\u001b[0;32m-\u003e 1257\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1259\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\n\u001b[0;32m/opt/spark/python/lib/pyspark.zip/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     67\u001b[0m                                              e.java_exception.getStackTrace()))\n\u001b[1;32m     68\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u0027org.apache.spark.sql.AnalysisException: \u0027\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---\u003e 69\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mAnalysisException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u0027: \u0027\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u0027org.apache.spark.sql.catalyst.analysis\u0027\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mAnalysisException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u0027: \u0027\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\n\u001b[0;31mAnalysisException\u001b[0m: \u0027Table or view not found: edr3_sources; line 1 pos 182\u0027"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1625860018274_-1211352710",
      "id": "20210709-194658_1026318834",
      "dateCreated": "2021-07-09 19:46:58.274",
      "dateStarted": "2021-07-09 20:03:14.747",
      "dateFinished": "2021-07-09 20:03:14.951",
      "status": "ERROR",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%md\n\n# Evalute all sources from raw_sources_cached using the given model\n",
      "user": "dcr",
      "dateUpdated": "2021-07-19 13:30:00.822",
      "config": {
        "colWidth": 12.0,
        "fontSize": 9.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true,
          "completionKey": "TAB",
          "completionSupport": false
        },
        "editorMode": "ace/mode/markdown",
        "editorHide": true,
        "tableHide": false
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "\u003cdiv class\u003d\"markdown-body\"\u003e\n\u003ch1\u003eEvalute all sources from raw_sources_cached using the given model\u003c/h1\u003e\n\u003c/div\u003e"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1626167903323_406401056",
      "id": "20210713-091823_1640482153",
      "dateCreated": "2021-07-13 09:18:23.323",
      "dateStarted": "2021-07-19 13:30:00.822",
      "dateFinished": "2021-07-19 13:30:00.831",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "title": "Load models",
      "text": "%spark.pyspark\ndef loadScaler(filename, scalerType \u003d \u0027standard\u0027):\n    \u0027\u0027\u0027loads scaler based of scaler type\u0027\u0027\u0027\n    if scalerType \u003d\u003d \u0027standard\u0027:\n        from pyspark.ml.feature import StandardScalerModel\n        model \u003d StandardScalerModel.load(filename)\n    \n    elif scaterType \u003d\u003d \u0027MinMax\u0027:\n        from pyspark.ml.feature import MinMaxScalerModel\n        model \u003d MinMaxScalerModel.load(filename)\n    else: raise NameError(f\u0027scalerType can be either \"MinMax\" or \"standard\" not {scalerType}\u0027)\n    return model\n        \ndef nnLoadModel(filepath):\n    \u0027\u0027\u0027loads NN from savefile\u0027\u0027\u0027\n    from pyspark.ml.classification import MultilayerPerceptronClassificationModel\n    return MultilayerPerceptronClassificationModel.load(filepath) ",
      "user": "dcr",
      "dateUpdated": "2021-07-19 13:51:01.771",
      "config": {
        "colWidth": 12.0,
        "fontSize": 9.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "python",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "editorMode": "ace/mode/python",
        "title": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "apps": [],
      "jobName": "paragraph_1626167902558_-301626373",
      "id": "20210713-091822_341278603",
      "dateCreated": "2021-07-13 09:18:22.558",
      "status": "READY",
      "progressUpdateIntervalMs": 500
    },
    {
      "title": "Evaluate Unlabelled Data",
      "text": "%spark.pyspark\n\ndef selectRegime(df, regime \u003d \u0027high\u0027):\n    if \u0027high\u0027 in regime:\n        return df.filter(col(\u0027abs_SNR\u0027) \u003e 4.5)\n    if \u0027low\u0027 in regime:\n        return df.filter(col(\u0027abs_SNR\u0027) \u003c 4.5)\n\n\ndef select_unlabelled_data(table, regime, filename, featuresCol \u003d \u0027features\u0027, \n                           normalise \u003d False, scalerType \u003d \u0027standard\u0027):\n    \u0027\u0027\u0027selects full table to apply the trained model\u0027\u0027\u0027\n    \n    features \u003d getFeatures(withPhotometric \u003d True)\n    # Get unlabelled data\n    df \u003d spark.sql(f\u0027SELECT a.*, a.phot_g_mean_mag + 5.0*LOG10(a.parallax/100.0) \\\n                                         AS m_g, a.g_rp FROM {table} as a\u0027)\n    df \u003d df.select(features).withColumn(\u0027abs_SNR\u0027, \n                        f.abs(col(\u0027parallax_over_error\u0027)))\n    \n    # Get data that fits the regime abs_SNR parameters\n    df \u003d selectRegime(df \u003d df, regime \u003d regime)\n    features \u003d select_training_features(df, regime \u003d regime)\n    \n    # Assemble training features into DenseVector\n    assembler \u003d VectorAssembler(inputCols\u003dfeatures,\n                                outputCol\u003dfeaturesCol)\n    df \u003d assembler.transform(df)\\\n                .select([\u0027source_id\u0027, \u0027m_g\u0027, \u0027g_rp\u0027, featuresCol])\n    \n    if normalise:\n        # Scale the independent test data with a loaded scaler\n        scaler \u003d loadScaler(f\u0027{filename}/{scalerType}_normalisation_model\u0027, scalerType \u003d scalerType)\n        df \u003d normaliseData(df.withColumnRenamed(featuresCol, \u0027inputFeatures\u0027), model \u003d scaler)\n        \n    # Evaluate data with trained model\n    nnModelRead \u003d nnLoadModel(filepath \u003d f\u0027{filename}/MultilayerPerceptronClassifier\u0027)\n    df \u003d evaluateModel(nnModel \u003d nnModelRead, df \u003d df, normalise \u003d True,  evaluate \u003d False, \n                       labelCol \u003d \u0027is_good\u0027)\n    return df\n\nnew \u003d select_unlabelled_data(table \u003d \u0027edr3_sources\u0027, regime \u003d \u0027high\u0027, filename \u003d \u0027temp/\u0027,\n                       normalise \u003d True, scalerType \u003d \u0027standard\u0027)\n\nnew.createOrReplaceTempView(\u0027dcr_ML_res\u0027)",
      "user": "dcr",
      "dateUpdated": "2021-07-19 13:51:40.430",
      "config": {
        "colWidth": 12.0,
        "fontSize": 9.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "python",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "editorMode": "ace/mode/python",
        "title": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "apps": [],
      "jobName": "paragraph_1626167901869_-1328645231",
      "id": "20210713-091821_219105253",
      "dateCreated": "2021-07-13 09:18:21.869",
      "status": "READY",
      "progressUpdateIntervalMs": 500
    },
    {
      "title": "Spatial Plots",
      "text": "%spark.pyspark\ndef spatialDistributionSubplot(catalogue, resCat, class_val \u003d 0, label \u003d None, color \u003d \u0027k\u0027, save \u003d False):\n\n    plt.figure(3, figsize \u003d (16.18, 10.0))\n    \n    for i, value in enumerate(class_val):\n        plt.subplot(1,2,i+1, projection\u003d\u0027aitoff\u0027)\n        plt.grid(True)\n\n        df \u003d spark.sql(f\u0027SELECT b.prediction , b.source_id,\\\n                                                 a.ra, a.dec FROM {catalogue} as a, \\\n                                                 {resCat} as b \\\n                                                 WHERE a.source_id \u003d b.source_id \\\n                                                 AND b.prediction \u003d {class_val[i]}\u0027)\n        print(df.count())\n        x \u003d list((df.select(\u0027ra\u0027).toPandas()[\u0027ra\u0027] - 180.0) * np.pi / 180.0)\n        y \u003d list( df.select(\u0027dec\u0027).toPandas()[\u0027dec\u0027] * np.pi / 180.0)\n        plt.title(f\u0027{label[i]}\u0027)\n        plt.scatter(x, y, marker \u003d \u0027.\u0027, s \u003d 1, c \u003d color[i])\n    if save:\n        savefigs(save)\n    plt.show()\n    \nspatialDistributionSubplot(catalogue \u003d \u0027edr3_sources\u0027, resCat \u003d \u0027dcr_ML_res\u0027, \n                    class_val \u003d [1,0], label \u003d [\u0027Good Sources\u0027, \u0027Bad Sources\u0027], \n                    color \u003d [\u0027k\u0027, \u0027darkred\u0027])",
      "user": "dcr",
      "dateUpdated": "2021-07-19 13:51:29.364",
      "config": {
        "colWidth": 12.0,
        "fontSize": 9.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "python",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "editorMode": "ace/mode/python",
        "title": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "apps": [],
      "jobName": "paragraph_1626167900973_-342336082",
      "id": "20210713-091820_414334511",
      "dateCreated": "2021-07-13 09:18:20.973",
      "status": "READY",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%spark.pyspark\n",
      "user": "dcr",
      "dateUpdated": "2021-07-13 09:18:18.457",
      "config": {},
      "settings": {
        "params": {},
        "forms": {}
      },
      "apps": [],
      "jobName": "paragraph_1626167898455_1167035981",
      "id": "20210713-091818_1065685626",
      "dateCreated": "2021-07-13 09:18:18.455",
      "status": "READY",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%spark.pyspark\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n",
      "user": "dcr",
      "dateUpdated": "2021-07-13 09:18:30.403",
      "config": {
        "colWidth": 12.0,
        "fontSize": 9.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "python",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "editorMode": "ace/mode/python"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "apps": [],
      "jobName": "paragraph_1625860067491_920688105",
      "id": "20210709-194747_1351702772",
      "dateCreated": "2021-07-09 19:47:47.492",
      "status": "READY",
      "progressUpdateIntervalMs": 500
    }
  ],
  "name": "experiments/dcr/ML_cuts",
  "id": "2G9BXYCKP",
  "noteParams": {},
  "noteForms": {},
  "angularObjects": {
    "md:shared_process": [],
    "python:shared_process": [],
    "sh:shared_process": [],
    "spark:shared_process": []
  },
  "config": {
    "isZeppelinNotebookCronEnable": false,
    "looknfeel": "default",
    "personalizedMode": "false"
  },
  "info": {}
}