{
  "paragraphs": [
    {
      "text": "%md\nSee [this link](https://arxiv.org/abs/1907.07709).\n\n",
      "user": "gaiauser",
      "dateUpdated": "2020-04-27 13:01:36.363",
      "config": {
        "tableHide": false,
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true,
          "completionKey": "TAB",
          "completionSupport": false
        },
        "colWidth": 12.0,
        "editorMode": "ace/mode/markdown",
        "fontSize": 9.0,
        "editorHide": true,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "\u003cdiv class\u003d\"markdown-body\"\u003e\n\u003cp\u003eSee \u003ca href\u003d\"https://arxiv.org/abs/1907.07709\"\u003ethis link\u003c/a\u003e.\u003c/p\u003e\n\u003c/div\u003e"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1586451129225_1235931048",
      "id": "20200310-141359_226957878",
      "dateCreated": "2020-04-09 16:52:09.225",
      "dateStarted": "2020-04-27 13:01:36.363",
      "dateFinished": "2020-04-27 13:01:37.663",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%sh pip install pandas\n# -- ONLY run if error importing pandas! Must restart notebook if install required.\n",
      "user": "gaiauser",
      "dateUpdated": "2020-04-27 13:01:40.113",
      "config": {
        "editorSetting": {
          "language": "sh",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": false
        },
        "colWidth": 12.0,
        "editorMode": "ace/mode/sh",
        "fontSize": 9.0,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "DEPRECATION: Python 2.7 will reach the end of its life on January 1st, 2020. Please upgrade your Python as Python 2.7 won\u0027t be maintained after that date. A future version of pip will drop support for Python 2.7.\nRequirement already satisfied: pandas in /usr/lib64/python2.7/site-packages (0.24.2)\nRequirement already satisfied: pytz\u003e\u003d2011k in /usr/lib/python2.7/site-packages (from pandas) (2019.3)\nRequirement already satisfied: numpy\u003e\u003d1.12.0 in /usr/lib64/python2.7/site-packages (from pandas) (1.16.6)\nRequirement already satisfied: python-dateutil\u003e\u003d2.5.0 in /usr/lib/python2.7/site-packages (from pandas) (2.8.1)\nRequirement already satisfied: six\u003e\u003d1.5 in /usr/lib/python2.7/site-packages (from python-dateutil\u003e\u003d2.5.0-\u003epandas) (1.14.0)\n"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1586451129231_-1405256604",
      "id": "20200409-101627_1921515986",
      "dateCreated": "2020-04-09 16:52:09.231",
      "dateStarted": "2020-04-27 13:01:40.130",
      "dateFinished": "2020-04-27 13:01:42.793",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%spark.pyspark\n\n# define the data frame source on the given column selection/predicates:\ndf \u003d sqlContext.read.parquet(\n    \"/hadoop/gaia/parquet/gdr2/gaia_source/*.parquet\"\n    ).select(\n    [\"designation\",\"source_id\",\"ra\",\"ra_error\",\"dec\",\"dec_error\",\"parallax\",\"parallax_error\",\"parallax_over_error\",\"pmra\",\"pmra_error\",\"pmdec\",\"pmdec_error\",\"l\",\"b\"]\n    ).where(\n    \"abs(b) \u003c 30.0 AND parallax \u003e 1.0 and parallax_over_error \u003e 10.0 AND phot_g_mean_flux_over_error \u003e 36.19 AND astrometric_sigma5d_max \u003c 0.3 AND visibility_periods_used \u003e 8 AND (astrometric_excess_noise \u003c 1 OR (astrometric_excess_noise \u003e 1 AND astrometric_excess_noise_sig \u003c 2))\"\n    )\n\n# sanity check\ndf.show()\nprint (\"Data frame rows: \",df.count())\n\n\n\n\n",
      "user": "gaiauser",
      "dateUpdated": "2020-04-24 09:40:52.978",
      "config": {
        "editorSetting": {
          "language": "python",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "colWidth": 12.0,
        "editorMode": "ace/mode/python",
        "fontSize": 9.0,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "ERROR",
        "msg": [
          {
            "type": "TEXT",
            "data": "+--------------------+-------------------+------------------+--------------------+-------------------+--------------------+------------------+--------------------+-------------------+--------------------+--------------------+-------------------+--------------------+------------------+-------------------+\n|         designation|          source_id|                ra|            ra_error|                dec|           dec_error|          parallax|      parallax_error|parallax_over_error|                pmra|          pmra_error|              pmdec|         pmdec_error|                 l|                  b|\n+--------------------+-------------------+------------------+--------------------+-------------------+--------------------+------------------+--------------------+-------------------+--------------------+--------------------+-------------------+--------------------+------------------+-------------------+\n|Gaia DR2 40396662...|4039666296672658688|272.42180060744215|0.040014912399424604|  -33.6058067001021| 0.03368056050988915| 1.422454370334301| 0.03639988616495275|           39.07854| -0.2316953688797376| 0.07537877095615261| -5.442599071096166| 0.05727019630225524|358.50569414878646|-6.7818037118031835|\n|Gaia DR2 40396673...|4039667327461865216|272.65050482164776|0.036197874417770184| -33.67244054531187| 0.03680133647540661|1.1673137239965692| 0.05314058363370944|           21.96652| -1.7009801509860536| 0.07029452441830816| -4.612361250689048| 0.06085092511704771|358.53690441856696| -6.981138715828702|\n|Gaia DR2 40396638...|4039663822768546432| 272.3987836204827|0.027632276638390182|-33.677978944324124|0.024215370325550427| 1.485547221111856|0.028698903012696158|          51.763206|  -4.537954548081503|0.053742838174843835|-15.585752841174445|0.041699306844425514| 358.4325162212831| -6.799004597798456|\n|Gaia DR2 40396743...|4039674302495016832| 272.6370332645537|0.049971393914832546| -33.46345401655811| 0.04311907194434805|1.4895944391179747| 0.05535383094438027|          26.910412|  -8.999004938426811| 0.08901429663519801|-20.520372339141726| 0.07278953875661565| 358.7173313727555| -6.872875125255238|\n|Gaia DR2 40396798...|4039679804475284224| 272.3352958925287|  0.0382698356897684| -33.49684273645387|   0.037562716485539|1.8288406168863958|0.049118014784077936|            37.2336|  -8.709127050643408| 0.07206191453025368| 0.8317008127357967| 0.05431303815619029|  358.568061282215|  -6.66676412739173|\n|Gaia DR2 40396768...|4039676845112356736| 272.3423038848421| 0.04004613278050688| -33.60277422977108| 0.03534959354180454| 1.331757953116398|0.040852709757309896|           32.59901| -3.0736631636887344| 0.07887016066616566|-10.535097449648152| 0.06167703651701124|358.47687324908384| -6.722021120385618|\n|Gaia DR2 40396799...|4039679976215199616| 272.3870770186676| 0.08175560039135911| -33.49054936039203|   0.075806277430834|1.6828015366409843| 0.09616438158987135|          17.499218|  0.9078068372385885|    0.15609879018991|-1.2743414940340876| 0.12356941230805032| 358.5942080511929|  -6.70183491892182|\n|Gaia DR2 40396779...|4039677948992976896| 272.2946282491588| 0.12673632257759757|-33.562874682533035| 0.11352057025821762|1.2791078599019605| 0.12717607561652478|          10.057771|  2.4788330269377497|  0.2613657929145823| -4.650451848991806| 0.20943502287503454| 358.4933431825897| -6.668151666197993|\n|Gaia DR2 40396646...|4039664686186893184| 272.4026652887885|0.028595834471134035|-33.655006326453886| 0.02471972176840793|1.4230023257454893|0.029119862707883994|           48.86707|  -4.179903049739455|0.055444952571015844| 2.2287270254996123|  0.0428403985184529| 358.4544442439153| -6.791000148338951|\n|Gaia DR2 40396753...|4039675333287149952|  272.560428244972| 0.14485939823543023| -33.41907979107032| 0.13639184748166955|2.4892891798856938| 0.14768574894726139|           16.85531|   2.753665468378671|  0.2764468222685262| -6.187195733058044|  0.2210087660210865|358.72641382590615| -6.795584005240619|\n|Gaia DR2 40396696...|4039669629564951040| 272.7381011527819|0.054428306834246126| -33.56432279582611| 0.04383693249181095|1.4039064198829385| 0.06633427846464744|          21.164116|   3.411131784353724| 0.09135111973361876| -9.646276318124418| 0.08216672365634985| 358.6675940515556| -6.994684164951169|\n|Gaia DR2 40396626...|4039662693254567936| 272.5126080410794|  0.1296423098745303| -33.65215003086476| 0.10917065938921705|1.2421645377888288|  0.1122515594329213|            11.0659|    7.70196276687754| 0.25578905728633683|-17.583724821676032| 0.20088518616997741| 358.5004815041314| -6.870335004176007|\n|Gaia DR2 40396725...|4039672520147052672|272.57023824221136|  0.0736686882516885| -33.51129861096017| 0.06447426562797175| 1.186762299958647| 0.08862786004612552|          13.390398|   2.632117122829801| 0.13686967280780654| 0.3072041918179895|  0.1083246074218071|358.64839146395377| -6.846275843225122|\n|Gaia DR2 40396727...|4039672721944070272| 272.5831112426574| 0.07043976875751465|-33.483537099952784| 0.06291530701296215|  1.14022685446635| 0.08033686117429235|          14.193072|  0.5481922043982299|   0.129302666462865|-3.2025426136016226| 0.10287713081563385|358.67814785261555|-6.8426601585483455|\n|Gaia DR2 40396731...|4039673134263578112|272.47253543660037|0.041702049821903266| -33.51891353312088| 0.03645176703075055|1.1267320071311797|0.047071092260350056|          23.936815|  -1.641507160720163| 0.07769872412607842| -4.671616157042623| 0.06032524035452944| 358.6029327525877| -6.778040244101522|\n|Gaia DR2 40396646...|4039664686186892160|272.40377537872985| 0.04377600533015369| -33.65486867142623|0.037860082819238965|  1.75887165621122|  0.0445294022348985|          39.499107| -0.9566730190113414| 0.08496727540942774| -3.746117897740997|   0.066416625862686|358.45500600935554|  -6.79174955438742|\n|Gaia DR2 40396815...|4039681591181083520| 272.1513758092714| 0.03374330430351806| -33.54770223115408| 0.03215174942793003|1.2770089043898252| 0.03768576974690001|          33.885704| -14.528880635531543| 0.06242652192511705|-1.5617009216815694| 0.05084214045259791|  358.449834180826| -6.555840447573484|\n|Gaia DR2 40396683...|4039668396976095232| 272.5918785069274| 0.06755164731288285| -33.62795490984999|0.060064042374027175| 1.278165908009941| 0.08163770487003097|          15.656564|  -9.059932437739432| 0.12441941818286029| 1.0153309974276419| 0.10066735403770635|358.55330353126215| -6.917141240695328|\n|Gaia DR2 40396782...|4039678253864546816| 272.2796609111014| 0.12881518153071386| -33.51657315885373| 0.11029532166270983|1.1790261056103815| 0.11170275683028853|          10.555032|  -9.837527608593797|  0.2374980846729896| -4.101407578324116|  0.1907102657065912|358.52845560549713| -6.635240808816544|\n|Gaia DR2 40396815...|4039681591181086848| 272.1520450526198| 0.04506019510179999| -33.54255938796362| 0.04309540098315705|1.0121130032598475| 0.04860285626079909|          20.824146|-0.01420160186318746| 0.08474653494512939|-4.1072215733651944| 0.07023090482212226|358.45465735183967| -6.553891056963974|\n+--------------------+-------------------+------------------+--------------------+-------------------+--------------------+------------------+--------------------+-------------------+--------------------+--------------------+-------------------+--------------------+------------------+-------------------+\nonly showing top 20 rows\n\n"
          },
          {
            "type": "TEXT",
            "data": "Py4JJavaError: An error occurred while calling o377.count.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 5 in stage 30.0 failed 4 times, most recent failure: Lost task 5.3 in stage 30.0 (TID 79965, stv-dev-worker-4, executor 6): java.io.FileNotFoundException: /tmp/hadoop-fedora/nm-local-dir/usercache/fedora/appcache/application_1578764470640_0080/blockmgr-4717fa69-6726-40e4-b308-6c9a2b6c6c94/13/temp_shuffle_2d53591f-dba1-4924-b101-450c2e213a62 (No such file or directory)\n\tat java.io.FileOutputStream.open0(Native Method)\n\tat java.io.FileOutputStream.open(FileOutputStream.java:270)\n\tat java.io.FileOutputStream.\u003cinit\u003e(FileOutputStream.java:213)\n\tat org.apache.spark.storage.DiskBlockObjectWriter.initialize(DiskBlockObjectWriter.scala:103)\n\tat org.apache.spark.storage.DiskBlockObjectWriter.open(DiskBlockObjectWriter.scala:116)\n\tat org.apache.spark.storage.DiskBlockObjectWriter.write(DiskBlockObjectWriter.scala:237)\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:151)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:55)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:123)\n\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1889)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1877)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1876)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1876)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)\n\tat scala.Option.foreach(Option.scala:257)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:926)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2110)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2059)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2048)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:737)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2061)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2082)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2101)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2126)\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:945)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:363)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:944)\n\tat org.apache.spark.sql.execution.SparkPlan.executeCollect(SparkPlan.scala:299)\n\tat org.apache.spark.sql.Dataset$$anonfun$count$1.apply(Dataset.scala:2836)\n\tat org.apache.spark.sql.Dataset$$anonfun$count$1.apply(Dataset.scala:2835)\n\tat org.apache.spark.sql.Dataset$$anonfun$52.apply(Dataset.scala:3370)\n\tat org.apache.spark.sql.execution.SQLExecution$$anonfun$withNewExecutionId$1.apply(SQLExecution.scala:78)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:125)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:73)\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3369)\n\tat org.apache.spark.sql.Dataset.count(Dataset.scala:2835)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: java.io.FileNotFoundException: /tmp/hadoop-fedora/nm-local-dir/usercache/fedora/appcache/application_1578764470640_0080/blockmgr-4717fa69-6726-40e4-b308-6c9a2b6c6c94/13/temp_shuffle_2d53591f-dba1-4924-b101-450c2e213a62 (No such file or directory)\n\tat java.io.FileOutputStream.open0(Native Method)\n\tat java.io.FileOutputStream.open(FileOutputStream.java:270)\n\tat java.io.FileOutputStream.\u003cinit\u003e(FileOutputStream.java:213)\n\tat org.apache.spark.storage.DiskBlockObjectWriter.initialize(DiskBlockObjectWriter.scala:103)\n\tat org.apache.spark.storage.DiskBlockObjectWriter.open(DiskBlockObjectWriter.scala:116)\n\tat org.apache.spark.storage.DiskBlockObjectWriter.write(DiskBlockObjectWriter.scala:237)\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:151)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:55)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:123)\n\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\n\n(\u003cclass \u0027py4j.protocol.Py4JJavaError\u0027\u003e, Py4JJavaError(u\u0027An error occurred while calling o377.count.\\n\u0027, JavaObject id\u003do378), \u003ctraceback object at 0x7f1253dcd4b0\u003e)"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1586451129231_1401954213",
      "id": "20200310-140633_496300561",
      "dateCreated": "2020-04-09 16:52:09.231",
      "dateStarted": "2020-04-24 09:40:53.002",
      "dateFinished": "2020-04-24 09:41:00.481",
      "status": "ERROR",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%spark.pyspark\n\n# test: can we write to the file store?\n\n# Create some sample text, and turn into an RDD\nsometext\u003d\"TEST\"\nrdd \u003d sc.parallelize([sometext])\n\n# Then save as text file , using below if underline storage is HDFS\nrdd.saveAsTextFile(\"hdfs:///hadoop/temp/testN.txt\")\n\n# Read Text File we just wrote into a new rdd and print\nmy_rdd \u003d sc.textFile(\"hdfs:///hadoop/temp/testN.txt\")\n\nprint(my_rdd)\nmy_rdd.collect()\t\n\n# YES!\n",
      "user": "admin",
      "dateUpdated": "2020-04-09 18:17:32.616",
      "config": {
        "editorSetting": {
          "language": "python",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "colWidth": 12.0,
        "editorMode": "ace/mode/python",
        "fontSize": 9.0,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "apps": [],
      "jobName": "paragraph_1586451129232_1460454326",
      "id": "20200407-155926_429939943",
      "dateCreated": "2020-04-09 16:52:09.232",
      "dateStarted": "2020-04-09 18:17:32.651",
      "dateFinished": "2020-04-09 18:19:51.913",
      "status": "ERROR",
      "errorMessage": "",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%spark.pyspark\n\nimport pandas as pd\n\n\n# df.select(\u0027designation\u0027).show()",
      "user": "admin",
      "dateUpdated": "2020-04-09 21:27:32.533",
      "config": {
        "editorSetting": {
          "language": "python",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "colWidth": 12.0,
        "editorMode": "ace/mode/python",
        "fontSize": 9.0,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": []
      },
      "apps": [],
      "jobName": "paragraph_1586451129232_-589882942",
      "id": "20200409-093457_1719054744",
      "dateCreated": "2020-04-09 16:52:09.232",
      "dateStarted": "2020-04-09 21:27:32.550",
      "dateFinished": "2020-04-09 21:27:32.556",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%spark.pyspark\n#df \u003d pd.DataFrame(df, columns\u003ddf.columns)\n#print(type(df))\n#df.head()\n# ... doesn\u0027t work for me for the system install of pandas. Try this:\npandas_df \u003d df.select(\"*\").toPandas()\nprint(type(pandas_df))\npandas_df.head()\n\n",
      "user": "admin",
      "dateUpdated": "2020-04-09 21:27:44.733",
      "config": {
        "colWidth": 12.0,
        "fontSize": 9.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "python",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "editorMode": "ace/mode/python"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "\u003cclass \u0027pandas.core.frame.DataFrame\u0027\u003e\n                    designation            source_id  ...           l         b\n0  Gaia DR2 4039666296672658688  4039666296672658688  ...  358.505694 -6.781804\n1  Gaia DR2 4039667327461865216  4039667327461865216  ...  358.536904 -6.981139\n2  Gaia DR2 4039663822768546432  4039663822768546432  ...  358.432516 -6.799005\n3  Gaia DR2 4039674302495016832  4039674302495016832  ...  358.717331 -6.872875\n4  Gaia DR2 4039679804475284224  4039679804475284224  ...  358.568061 -6.666764\n\n[5 rows x 15 columns]\n"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1586452842691_637930684",
      "id": "20200409-172042_1802953724",
      "dateCreated": "2020-04-09 17:20:42.691",
      "dateStarted": "2020-04-09 21:27:44.751",
      "dateFinished": "2020-04-09 21:38:30.161",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%spark.pyspark\n\n#import csv\nimport time\nimport numpy as np\n#import pandas as pd\nfrom numpy import cos,sin,pi\n\n\u0027\u0027\u0027Converts the proper motions to km/s and performs a cut of LSR velocity \u003c 60km/s \u0027\u0027\u0027\n\n# converting proper motions (mas/yr --\u003e km/s)\npmra_kms \u003d np.array( pandas_df[\u0027pmra\u0027], dtype\u003dfloat)/(np.array( pandas_df[\u0027parallax\u0027],dtype\u003dfloat))*4.74057;\npmde_kms \u003d np.array(  pandas_df[\u0027pmdec\u0027], dtype\u003dfloat)/(np.array( pandas_df[\u0027parallax\u0027],dtype\u003dfloat))*4.74057;\n\n#insert into the df table\npandas_df.insert(7,\u0027pmra (km/s)\u0027, pmra_kms)\npandas_df.insert(8,\u0027pmdec (km/s)\u0027, pmde_kms)\n\n\n# convert to LSR Units\ndrop\u003d[]\nk\u003d4.74057\nv_sun \u003d np.array((11.1, 12.24, 7.25))\nT\u003d np.array([[-0.05646624, -0.87325802, -0.48397519],\n             [ 0.49253617, -0.44602111,  0.74731071],\n             [-0.86845823, -0.19617746,  0.4552963 ]])\n\ndef LSR_conv(ra,dec,parallax,pmra,pmdec,radial_velocity\u003d0.0):\n    a\u003dra*pi/180     # deg to rad\n    d\u003ddec*pi/180    # deg to rad\n    \n    # define a coordinate matrix\n    A \u003d np.array([[cos(a)*cos(d), -sin(a), -cos(a)*sin(d)],\n              [sin(a)*cos(d), cos(a), -sin(a)*sin(d)],\n              [sin(d), 0, cos(d)]])\n    \n    B \u003d np.matmul(T,A)\n    C \u003d np.array([radial_velocity, k*pmra/(parallax), k*pmdec/(parallax)])\n    U,V,W \u003d np.matmul(B,C)\n    u, v, w \u003d np.array((U,V,W)) + v_sun\n    return [u,v,w]\n\nfor i in range(len(pandas_df)):\n        ra,dec,parallax,pmra,pmdec\u003dnp.asarray(pandas_df.iloc[i][[\u0027ra\u0027,\u0027dec\u0027,\u0027parallax\u0027, \\\n                                                            \u0027pmra\u0027, \u0027pmdec\u0027]], dtype\u003dfloat)\n        x\u003dLSR_conv(ra,dec,parallax,pmra, pmdec)\n        if np.linalg.norm(x)\u003e60:\n            drop.append(i)\n\n# Drops the df if V_lsr \u003e60km/s\ndf\u003dpandas_df.drop(pandas_df.index[drop])\n\nprint(\u0027LSR cut complete\u0027)\n\n# write the new selection with appended columns to the file store to avoid needing to recompute:\n#df.rdd.saveAsTextFile(\"hdfs:///hadoop/temp/kc_hdbscan_inputs.txt\")",
      "user": "admin",
      "dateUpdated": "2020-04-09 21:38:35.429",
      "config": {
        "editorSetting": {
          "language": "python",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "colWidth": 12.0,
        "editorMode": "ace/mode/python",
        "fontSize": 9.0,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "LSR cut complete\n"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1586451129233_-1041364310",
      "id": "20200310-142111_1877616625",
      "dateCreated": "2020-04-09 16:52:09.233",
      "dateStarted": "2020-04-09 21:38:35.453",
      "dateFinished": "2020-04-10 02:48:28.764",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%spark.pyspark\n\nimport hdbscan\nfrom joblib import Memory\nmem \u003d Memory(cachedir\u003d\u0027hdfs:///hadoop/temp/\u0027)\n\u0027\u0027\u0027 Runs HDBSCAN on the data with predictive data enabled allowing the prediction of WD\u0027s\u0027\u0027\u0027\n\ndf\u003ddf.iloc[:][[\u0027source_id\u0027,\u0027l\u0027, \u0027b\u0027,\u0027ra\u0027,\u0027dec\u0027,\u0027parallax\u0027,\u0027pmra (km/s)\u0027,\u0027pmdec (km/s)\u0027]]\n# df.to_csv(\"df/DR2_with_cuts.csv\", index\u003dNone)\n\n\n# WDs \u003d pd.read_csv(\u0027df/WDs_clustering.csv\u0027)     # White dwarf data\n\n#print(df.shape)\n\n# Apply HDBSCAN with prediction ON full df\ncluster_df \u003d df[[\u0027l\u0027, \u0027b\u0027, \u0027parallax\u0027,\u0027pmra (km/s)\u0027,\u0027pmdec (km/s)\u0027]]\nprint(cluster_df)\nclusterer \u003d hdbscan.HDBSCAN(min_cluster_size\u003d40, \n                            min_samples\u003d2,\n                            prediction_df\u003dTrue, \n                            allow_single_cluster\u003dFalse,\n                            memory\u003dmem,           # Saved to local storage \u0027None\u0027 if this is not possible\n                            cluster_selection_method\u003d\u0027leaf\u0027,\n                            gen_min_span_tree\u003dTrue).fit(cluster_df)\n\n\n# probabilities and group for each object\nprobabilities\u003dclusterer.probabilities_\ngroups\u003dclusterer.labels_\n\n# Info on persistence of groups\npersistence\u003dclusterer.cluster_persistence_\nprint(\u0027Number of Groups \u003d \u0027, max(groups)+1) \n\n\n# np.savetxt(\u0027Groups_persistance_leaf.csv\u0027, persistence)\ndf.insert(8,\u0027group\u0027, groups)\ndf.insert(9,\u0027probability\u0027, probabilities)\n\n# df.to_csv(\"DR2_with_groups_leaf.csv\", index\u003dNone)\nprint(\u0027clustering complete\u0027)\n\n\n# Uses the above clustering to predict WD associations\n\n# WD_cluster\u003dWDs[[\u0027l\u0027, \u0027b\u0027, \u0027Plx\u0027, \u0027pmra (km/s)\u0027, \u0027pmdec (km/s)\u0027]]\n# WD_labels, strengths \u003d hdbscan.approximate_predict(clusterer, WD_cluster)\n# WDs.insert(8,\u0027group\u0027, WD_labels)\n# WDs.insert(9,\u0027probability\u0027, strengths)\n# WDs.to_csv(\"WDs_with_groups_leaf.csv\", index\u003dNone)\n\nprint(\u0027WDs check complete\u0027)\n",
      "user": "admin",
      "dateUpdated": "2020-04-09 21:20:50.222",
      "config": {
        "editorSetting": {
          "language": "python",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "colWidth": 12.0,
        "editorMode": "ace/mode/python",
        "fontSize": 9.0,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "              l         b   parallax  pmra (km/s)  pmdec (km/s)\n0    358.505694 -6.781804   1.422454    -0.772164    -18.138383\n1    358.536904 -6.981139   1.167314    -6.907839    -18.731230\n2    358.432516 -6.799005   1.485547   -14.481190    -49.736118\n3    358.717331 -6.872875   1.489594   -28.638945    -65.305199\n4    358.568061 -6.666764   1.828841   -22.575082      2.155866\n5    358.476873 -6.722021   1.331758   -10.941114    -37.501084\n6    358.594208 -6.701835   1.682802     2.557356     -3.589909\n7    358.493343 -6.668152   1.279108     9.186936    -17.235288\n8    358.454444 -6.791000   1.423002   -13.924870      7.424750\n9    358.726414 -6.795584   2.489289     5.244045    -11.782815\n10   358.667594 -6.994684   1.403906    11.518367    -32.572576\n11   358.500482 -6.870335   1.242165    29.393605    -67.106149\n12   358.648391 -6.846276   1.186762    10.514098      1.227140\n13   358.678148 -6.842660   1.140227     2.279146    -13.314787\n14   358.602933 -6.778040   1.126732    -6.906416    -19.655183\n15   358.455006 -6.791750   1.758872    -2.578457    -10.096663\n16   358.449834 -6.555840   1.277009   -53.934765     -5.797417\n17   358.553304 -6.917141   1.278166   -33.602245      3.765746\n18   358.528456 -6.635241   1.179026   -39.554246    -16.490737\n19   358.454657 -6.553891   1.012113    -0.066518    -19.237547\n20   358.500087 -6.820080   1.146016    -4.957981     -0.020549\n21   358.474819 -6.558913   1.161654   -27.699660    -48.958188\n22   358.481986 -6.795992   1.321398    -0.994138    -23.675979\n23   358.513860 -6.631456   2.734638   -15.667215    -38.830896\n24   358.534856 -6.811587   1.481419   -35.285916    -32.249558\n25   358.691762 -6.982413   1.265479     5.165970    -13.813605\n26   358.599551 -6.837845   8.426658     0.888521    -36.672679\n27   358.462273 -6.544135   1.400488     9.057127    -15.943197\n28   358.548642 -6.801119   2.287142   -12.184507    -37.439813\n29   358.531296 -6.790016   1.290233     9.811377    -22.002808\n..          ...       ...        ...          ...           ...\n967    1.645714 -7.965796   1.210722   -31.117154    -34.104749\n968    1.756686 -7.979495   1.695560   -16.309442     14.226986\n969    1.450521 -8.049079   3.254857    23.223114     -8.953050\n970    1.450257 -8.048785   3.443726    22.308068     -7.454543\n971    1.802651 -7.945791   1.184704   -21.445031    -32.993105\n972    1.401864 -7.798301   1.063268    12.597153     -3.624023\n973    1.407590 -7.990187   1.218296    -6.084324    -60.068076\n974    1.797825 -7.941427   1.251029     7.947801    -39.463147\n975    1.381557 -7.846110   1.511972    21.561614     -3.173061\n977    1.489681 -7.814599   1.637323    26.504770      5.141929\n978    1.405391 -7.653149   1.357995    19.074060    -24.734342\n979    1.384262 -7.678391   1.111862    -1.689282    -11.877422\n980    1.431363 -7.683456   1.060636    -9.253377    -27.626528\n981    1.480751 -8.013516   1.148542     8.538932    -59.118269\n983    1.423753 -7.821427   1.012593   -12.498809    -20.701203\n984    1.447706 -7.862355   1.910167    -3.080213    -11.498664\n985    1.410388 -8.021391  14.375832    12.507292    -27.855533\n986    1.792233 -7.964940   1.095835    10.677669     16.945965\n987    1.437409 -7.742349  10.332828    -2.088039    -20.365352\n988    1.464636 -7.948671   1.120915     4.144629    -24.052191\n989    1.378050 -7.656700   1.119223     9.031112    -17.873233\n990    1.718586 -7.925315   1.090830    -1.636137     10.827011\n991    1.382437 -7.711992   1.375752    -7.150351    -33.821431\n992    1.472237 -7.815401   1.587859     3.484685    -26.887758\n993    1.374048 -7.905404   1.646067    -8.042776     14.637778\n994    1.518570 -8.082213   2.130867     3.585382    -22.265722\n995    1.352693 -7.677183   1.344316    -9.341661    -22.988938\n997    1.658290 -8.034329   1.135129    15.598130    -18.479051\n998    1.455096 -8.071273   1.600873    11.914252    -15.535146\n999    1.745498 -8.018510   1.009264    27.176538     -5.972810\n\n[914 rows x 5 columns]\n________________________________________________________________________________\n[Memory] Calling hdbscan.hdbscan_._hdbscan_boruvka_kdtree...\n_hdbscan_boruvka_kdtree(array([[358.505694, ..., -18.138383],\n       ...,\n       [  1.745498, ...,  -5.97281 ]]), \n2, 1.0, \u0027euclidean\u0027, None, 40, True, True, 4)\n___________________________________________hdbscan_boruvka_kdtree - 0.0s, 0.0min\n(\u0027Number of Groups \u003d \u0027, 5)\nclustering complete\nWDs check complete\n"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1586451129233_981598480",
      "id": "20200319-104018_1699124593",
      "dateCreated": "2020-04-09 16:52:09.233",
      "dateStarted": "2020-04-09 21:05:27.995",
      "dateFinished": "2020-04-09 21:05:28.051",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%spark.pyspark\n",
      "user": "admin",
      "dateUpdated": "2020-04-09 19:51:46.895",
      "config": {},
      "settings": {
        "params": {},
        "forms": {}
      },
      "apps": [],
      "jobName": "paragraph_1586461906893_-1758242593",
      "id": "20200409-195146_1906830473",
      "dateCreated": "2020-04-09 19:51:46.894",
      "status": "READY",
      "progressUpdateIntervalMs": 500
    }
  ],
  "name": "~Trash/stv-hdbscan-2",
  "id": "2F85DT5Z5",
  "noteParams": {},
  "noteForms": {},
  "angularObjects": {},
  "config": {
    "isZeppelinNotebookCronEnable": false
  },
  "info": {}
}