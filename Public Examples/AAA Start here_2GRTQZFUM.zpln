{
  "paragraphs": [
    {
      "title": "Introduction",
      "text": "%md\n\n<!--\n\n    Gaia Data Processing and Analysis Consortium (DPAC) \n    Co-ordination Unit 9 Work Package 930\n    \n    (c) 2005-2025 Gaia DPAC\n    \n    This program is free software: you can redistribute it and/or modify\n    it under the terms of the GNU General Public License as published by\n    the Free Software Foundation, either version 3 of the License, or\n    (at your option) any later version.\n\n    This program is distributed in the hope that it will be useful,\n    but WITHOUT ANY WARRANTY; without even the implied warranty of\n    MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n    GNU General Public License for more details.\n\n    You should have received a copy of the GNU General Public License\n    along with this program.  If not, see <https://www.gnu.org/licenses/>.\n    -->\n\nThis Gaia science exploitation platform is built on [Apache Spark](https://spark.apache.org). The primary user interface uses web-based [Zeppelin notebooks](https://zeppelin.apache.org/docs/0.9.0/quickstart/explore_ui.html#note-layout) (similar to the possibly more familiar Jupyter notebook interface) employing Python code and a suite of third-party packages and libraries for analysis, plotting, etc. A working knowledge of Gaia data releases to-date, Python and Structured Query Language is assumed.\n",
      "user": "nch",
      "dateUpdated": "2022-03-04T12:11:25+0000",
      "progress": 0,
      "config": {
        "tableHide": false,
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true,
          "completionKey": "TAB",
          "completionSupport": false
        },
        "colWidth": 12,
        "editorMode": "ace/mode/markdown",
        "fontSize": 9,
        "editorHide": true,
        "title": true,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "<div class=\"markdown-body\">\n<!--\n\n    Gaia Data Processing and Analysis Consortium (DPAC) \n    Co-ordination Unit 9 Work Package 930\n    \n    (c) 2005-2025 Gaia DPAC\n    \n    This program is free software: you can redistribute it and/or modify\n    it under the terms of the GNU General Public License as published by\n    the Free Software Foundation, either version 3 of the License, or\n    (at your option) any later version.\n\n    This program is distributed in the hope that it will be useful,\n    but WITHOUT ANY WARRANTY; without even the implied warranty of\n    MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n    GNU General Public License for more details.\n\n    You should have received a copy of the GNU General Public License\n    along with this program.  If not, see <https://www.gnu.org/licenses/>.\n    -->\n<p>This Gaia science exploitation platform is built on <a href=\"https://spark.apache.org\">Apache Spark</a>. The primary user interface uses web-based <a href=\"https://zeppelin.apache.org/docs/0.9.0/quickstart/explore_ui.html#note-layout\">Zeppelin notebooks</a> (similar to the possibly more familiar Jupyter notebook interface) employing Python code and a suite of third-party packages and libraries for analysis, plotting, etc. A working knowledge of Gaia data releases to-date, Python and Structured Query Language is assumed.</p>\n\n</div>"
          }
        ]
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1639345822509_453424572",
      "id": "20210507-152557_21014937",
      "dateCreated": "2021-12-12T21:50:22+0000",
      "dateStarted": "2022-03-04T12:05:19+0000",
      "dateFinished": "2022-03-04T12:05:19+0000",
      "status": "FINISHED",
      "focus": true,
      "$$hashKey": "object:34177"
    },
    {
      "title": "Familiarisation",
      "text": "%md\n\n<br>\nThe best way to familiarise yourself with the platform is to examine the example notebooks that come bundled with it. These are:\n\n* \"Start here\" (this notebook);\n* Data holdings: a description of the data hosted on the platform;\n* Source counts over the sky: a simple example of querying and plotting;\n* Mean proper motions over the sky: another simple example\n* __TODO__ Further technical aspects of the platform\n* Good astrometric solutions via ML Random Forest classifier: a more complex workflow demonstrating use of the Spark ML library in supervised machine learning;\n* __TODO__ XXXX: 5d spatial clustering using HDBSCAN (unsupervised ML)  \n\nTo load a notebook, click on the \"Notebook\" tab (above left) and select. Newcomers are advised to work through at least the first four notebooks before attempting to use the system. Also have a quick look at the Zeppelin link in the previous cell first to get oriented in the User Interface and see also the \"Zeppelin Tutorial\" folder under the Notebooks tab. ",
      "user": "anonymous",
      "dateUpdated": "2021-12-12T21:50:22+0000",
      "progress": 0,
      "config": {
        "tableHide": false,
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true,
          "completionKey": "TAB",
          "completionSupport": false
        },
        "colWidth": 12,
        "editorMode": "ace/mode/markdown",
        "fontSize": 9,
        "editorHide": true,
        "title": true,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "<div class=\"markdown-body\">\n<br>\n<p>The best way to familiarise yourself with the platform is to examine the example notebooks that come bundled with it. These are:</p>\n<ul>\n  <li>&ldquo;Start here&rdquo; (this notebook);</li>\n  <li>Data holdings: a description of the data hosted on the platform;</li>\n  <li>Source counts over the sky: a simple example of querying and plotting;</li>\n  <li>Mean proper motions over the sky: another simple example</li>\n  <li><strong>TODO</strong> Further technical aspects of the platform</li>\n  <li>Good astrometric solutions via ML Random Forest classifier: a more complex workflow demonstrating use of the Spark ML library in supervised machine learning;</li>\n  <li><strong>TODO</strong> XXXX: 5d spatial clustering using HDBSCAN (unsupervised ML)</li>\n</ul>\n<p>To load a notebook, click on the &ldquo;Notebook&rdquo; tab (above left) and select. Newcomers are advised to work through at least the first four notebooks before attempting to use the system. Also have a quick look at the Zeppelin link in the previous cell first to get oriented in the User Interface and see also the &ldquo;Zeppelin Tutorial&rdquo; folder under the Notebooks tab.</p>\n</div>"
          }
        ]
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1639345822509_1138439081",
      "id": "20210507-152628_446700578",
      "dateCreated": "2021-12-12T21:50:22+0000",
      "status": "READY",
      "$$hashKey": "object:34178"
    },
    {
      "title": "Zeppelin notebooks",
      "text": "%md\n\nThis and all the other example notebooks itemised above are [Zeppelin notebooks](). Text, code (in different languages) and plots can be freely mixed in the notebook in paragraphs, or cells. For example, to see the [mark-down code](https://sourceforge.net/p/zeppelin/wiki/markdown_syntax/) for this cell simply click on the outward-pointing arrows icon in the top right of the cell; click again to hide the code leaving only the output. Output is created by executing the cell: press the \"play\" icon in the top right. Other functionality is provided under the cog icon while buttons at the top of the web interface provide further management functions. To save a notebook locally, click on the \"Export this note\" icon at the top: doing so will create a local file with extension \".json\" (= JavaScript Object Notation format, which is not amenable to viewing/editing outside of the UI). To import a previously saved note (or indeed one provisioned externally) click on the Zeppelin icon in the top left of the UI and then click \"Import note\".\n\n__TODO__\n\n* Integration with github?\n* Sharing with others ...\n* ...\n",
      "user": "anonymous",
      "dateUpdated": "2021-12-12T21:50:22+0000",
      "progress": 0,
      "config": {
        "tableHide": false,
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true,
          "completionKey": "TAB",
          "completionSupport": false
        },
        "colWidth": 12,
        "editorMode": "ace/mode/markdown",
        "fontSize": 9,
        "editorHide": true,
        "title": true,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "<div class=\"markdown-body\">\n<p>This and all the other example notebooks itemised above are <a href=\"\">Zeppelin notebooks</a>. Text, code (in different languages) and plots can be freely mixed in the notebook in paragraphs, or cells. For example, to see the <a href=\"https://sourceforge.net/p/zeppelin/wiki/markdown_syntax/\">mark-down code</a> for this cell simply click on the outward-pointing arrows icon in the top right of the cell; click again to hide the code leaving only the output. Output is created by executing the cell: press the &ldquo;play&rdquo; icon in the top right. Other functionality is provided under the cog icon while buttons at the top of the web interface provide further management functions. To save a notebook locally, click on the &ldquo;Export this note&rdquo; icon at the top: doing so will create a local file with extension &ldquo;.json&rdquo; (= JavaScript Object Notation format, which is not amenable to viewing/editing outside of the UI). To import a previously saved note (or indeed one provisioned externally) click on the Zeppelin icon in the top left of the UI and then click &ldquo;Import note&rdquo;.</p>\n<p><strong>TODO</strong></p>\n<ul>\n  <li>Integration with github?</li>\n  <li>Sharing with others &hellip;</li>\n  <li>&hellip;</li>\n</ul>\n</div>"
          }
        ]
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1639345822509_48403428",
      "id": "20210510-084215_1445060106",
      "dateCreated": "2021-12-12T21:50:22+0000",
      "status": "READY",
      "$$hashKey": "object:34179"
    },
    {
      "title": "PySpark SQL",
      "text": "%md\n\nThe platform environment employs [Spark (version 2.4.7)](https://spark.apache.org/docs/2.4.7/) for distributed processing and scalability to high-volume data releases. For convenience a high-level application programming interface (API) is available through Python to access and analyse large data sets. The primary data transport type is known as a [\"data frame\"](https://spark.apache.org/docs/2.4.7/sql-programming-guide.html), a richly structured object encapsulating data, descriptions and methods that is _not_ limited to simple tables with primitive column types in contrast to systems built on relational data base technology (RDBMS). For ease of use the [API features Structured Query Language](https://spark.apache.org/docs/2.4.7/api/python/pyspark.sql.html) as a familiar way to create and manipulate data objects as the initial steps in work flows. Note that this is not Astronomy Data Query Language (ADQL): astronomy-specific aspects, in particular geometric functions for spatial queries, are not available in this flavour of SQL. However PySpark SQL offers a much greater degree of end-user programmability than ADQL interfaces built in front of RDBMS, for example [User Defined Functions](https://spark.apache.org/docs/2.4.7/api/python/pyspark.sql.html#pyspark.sql.UDFRegistration) that can be integrated into SQL statements. The fact that SQL features prominently in the API should not be taken to mean that this platform aims to replicate the functionality of existing relational systems. Obviously, usage scenarios best served by such systems should be targetted there. This platform is designed to facilitate scale-out usage scenarios, e.g. large-scale statistical analyses, that cannot be achieved through relational systems built on TAP/ADQL. (In fact on this platform you can mix-and-match both by calling out to any IVOA TAP/ADQL service as part of a larger work flow.)\n\nOn start-up and by default, notebooks come set up in a Spark and Gaia EDR3 context by default, e.g.\n\n    %pyspark\n    for line in spark.catalog.listTables(): print (line)\n\n... see the next code cell. The notebook interpreter can be set for individual cells as is done here and in the next: \"%md\" for mark-down, \"%pyspark\" for the PySpark python code etc.\n",
      "user": "anonymous",
      "dateUpdated": "2021-12-12T21:50:22+0000",
      "progress": 0,
      "config": {
        "tableHide": false,
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true,
          "completionKey": "TAB",
          "completionSupport": false
        },
        "colWidth": 12,
        "editorMode": "ace/mode/markdown",
        "fontSize": 9,
        "editorHide": true,
        "title": true,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "<div class=\"markdown-body\">\n<p>The platform environment employs <a href=\"https://spark.apache.org/docs/2.4.7/\">Spark (version 2.4.7)</a> for distributed processing and scalability to high-volume data releases. For convenience a high-level application programming interface (API) is available through Python to access and analyse large data sets. The primary data transport type is known as a <a href=\"https://spark.apache.org/docs/2.4.7/sql-programming-guide.html\">&ldquo;data frame&rdquo;</a>, a richly structured object encapsulating data, descriptions and methods that is <em>not</em> limited to simple tables with primitive column types in contrast to systems built on relational data base technology (RDBMS). For ease of use the <a href=\"https://spark.apache.org/docs/2.4.7/api/python/pyspark.sql.html\">API features Structured Query Language</a> as a familiar way to create and manipulate data objects as the initial steps in work flows. Note that this is not Astronomy Data Query Language (ADQL): astronomy-specific aspects, in particular geometric functions for spatial queries, are not available in this flavour of SQL. However PySpark SQL offers a much greater degree of end-user programmability than ADQL interfaces built in front of RDBMS, for example <a href=\"https://spark.apache.org/docs/2.4.7/api/python/pyspark.sql.html#pyspark.sql.UDFRegistration\">User Defined Functions</a> that can be integrated into SQL statements. The fact that SQL features prominently in the API should not be taken to mean that this platform aims to replicate the functionality of existing relational systems. Obviously, usage scenarios best served by such systems should be targetted there. This platform is designed to facilitate scale-out usage scenarios, e.g. large-scale statistical analyses, that cannot be achieved through relational systems built on TAP/ADQL. (In fact on this platform you can mix-and-match both by calling out to any IVOA TAP/ADQL service as part of a larger work flow.)</p>\n<p>On start-up and by default, notebooks come set up in a Spark and Gaia EDR3 context by default, e.g.</p>\n<pre><code>%pyspark\nfor line in spark.catalog.listTables(): print (line)\n</code></pre>\n<p>&hellip; see the next code cell. The notebook interpreter can be set for individual cells as is done here and in the next: &ldquo;%md&rdquo; for mark-down, &ldquo;%pyspark&rdquo; for the PySpark python code etc.</p>\n</div>"
          }
        ]
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1639345822509_903343457",
      "id": "20210507-160139_1365676170",
      "dateCreated": "2021-12-12T21:50:22+0000",
      "status": "READY",
      "$$hashKey": "object:34180"
    },
    {
      "title": "Example code from previous cell",
      "text": "%pyspark\nfor line in spark.catalog.listTables(): print (line)\n",
      "user": "anonymous",
      "dateUpdated": "2021-12-12T21:50:22+0000",
      "progress": 0,
      "config": {
        "editorSetting": {
          "language": "python",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "colWidth": 12,
        "editorMode": "ace/mode/python",
        "fontSize": 9,
        "editorHide": true,
        "title": true,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "Table(name='gaia_source', database='gaiaedr3', description=None, tableType='EXTERNAL', isTemporary=False)\nTable(name='gaia_source_allwise_best_neighbours', database='gaiaedr3', description=None, tableType='EXTERNAL', isTemporary=False)\nTable(name='gaia_source_ps1_best_neighbours', database='gaiaedr3', description=None, tableType='EXTERNAL', isTemporary=False)\nTable(name='gaia_source_tmasspsc_best_neighbours', database='gaiaedr3', description=None, tableType='EXTERNAL', isTemporary=False)\n"
          }
        ]
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1639345822509_2133917690",
      "id": "20210510-105824_1166659944",
      "dateCreated": "2021-12-12T21:50:22+0000",
      "status": "READY",
      "$$hashKey": "object:34181"
    },
    {
      "title": "Spark aspects",
      "text": "%md\n\nThe operating model of Spark as actioned via a workflow expressed in a notebook will be unfamiliar to many. Users should be aware of the following basic facts from the outset:\n\n__Lazy Evaluation:__ Spark employs \"lazy evaluation\" of jobs: nothing happens until the last possible point in the workflow. For example, a notebook cell may contain a data frame defined by a query \n\n    df = spark.sql(\"select * from gaia_source\")\n\nbut as far as Spark is concerned this is simply a definiton, or \"transformation\", of a data resource. If nothing is done with the data frame defined by that query in the cell, then the cell will appear to run instantaneously. Only if an \"action\" is made on the data frame subsequently will physical execution take place. This means, for example, that if a cell contains a run-time error this may not show up until a later cell is executed with the exception trace explaining the error appearing in that later cell's output (as opposed to any output associated with the cell containing the source of the error). Note also that the statement above, on it's own, will not result in the entire billon+ row, ~100 column, tera-byte scale data set being loaded into memory. Whatever is expressed in subsequent transformations (e.g. SQL \"where\" clauses) and actions (e.g. aggregations) will result is \"filter push-down\" optimisations that result in only those data needed to action the process being read into memory and processed.  \n\n__Caching:__ Actions expressed in a cell may need to be re-executed if needed as part of the execution plan for actions in a subsequent cell. This may result in expensive operations being repeatedly executed when developing notebook workflows and working on cells in isolation. You can tell Spark to cache a data frame explicitly to avoid this and speed-up significantly subsequent cell operations\n\n    df = spark.sql(\"select column_1, column_2, ... from gaia_source where ...\").cache()\n    \nbut obviously this should only be done within reason and with due regard to the amount of local memory/disk required by the worker nodes: make sure you only cache what you really need. For further details see [this useful article on best practice with Spark caching](https://towardsdatascience.com/best-practices-for-caching-in-spark-sql-b22fb0f02d34). Some of the example notebooks provided use caching to expedite the execution of cells that re-use data frames.\n\n__Indexing:__ The data hosted on this platform are arranged by gaia_source.source_id as the primary unique identifier in Gaia Data Releases. This key should be employed when joining distinct data sets, e.g. pre-computed best neighbours from external survey catalogues - see the \"Data holdings\" notebook. It can also be used to pull out specific records quickly:\n\n    spark.sql(\"select * from gaia_source where source_id = ...\").show()\n   \nOtherwise, the general RDBMS paradigm employing SQL indexes to optimise execution plans of queries predicated on certain columns has no equivalent on here. It is worth noting however that the combination of \"filter push-down\" and the underlying storage format ([Parquet](http://parquet.apache.org/documentation/latest/)) result in only those data that are needed being read from disk on this patform. This is in contrast to RDBMS where columns are generally read from pages within disk files regardless of need unless covered by an index employed as part of an optimised execution plan. This is one of the ways in which Spark achieves much faster performance in \"big data\" analysis.\n\n__Distributed processing model:__ Fast performance in handling very large data sets is achieved via cluster parallelism with actions split up and optimised for execution on concurrent workers... anything to be said here? \n\n",
      "user": "anonymous",
      "dateUpdated": "2021-12-12T21:50:22+0000",
      "progress": 0,
      "config": {
        "tableHide": false,
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true,
          "completionKey": "TAB",
          "completionSupport": false
        },
        "colWidth": 12,
        "editorMode": "ace/mode/markdown",
        "fontSize": 9,
        "editorHide": true,
        "title": true,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "<div class=\"markdown-body\">\n<p>The operating model of Spark as actioned via a workflow expressed in a notebook will be unfamiliar to many. Users should be aware of the following basic facts from the outset:</p>\n<p><strong>Lazy Evaluation:</strong> Spark employs &ldquo;lazy evaluation&rdquo; of jobs: nothing happens until the last possible point in the workflow. For example, a notebook cell may contain a data frame defined by a query </p>\n<pre><code>df = spark.sql(&quot;select * from gaia_source&quot;)\n</code></pre>\n<p>but as far as Spark is concerned this is simply a definiton, or &ldquo;transformation&rdquo;, of a data resource. If nothing is done with the data frame defined by that query in the cell, then the cell will appear to run instantaneously. Only if an &ldquo;action&rdquo; is made on the data frame subsequently will physical execution take place. This means, for example, that if a cell contains a run-time error this may not show up until a later cell is executed with the exception trace explaining the error appearing in that later cell&rsquo;s output (as opposed to any output associated with the cell containing the source of the error). Note also that the statement above, on it&rsquo;s own, will not result in the entire billon+ row, ~100 column, tera-byte scale data set being loaded into memory. Whatever is expressed in subsequent transformations (e.g. SQL &ldquo;where&rdquo; clauses) and actions (e.g. aggregations) will result is &ldquo;filter push-down&rdquo; optimisations that result in only those data needed to action the process being read into memory and processed. </p>\n<p><strong>Caching:</strong> Actions expressed in a cell may need to be re-executed if needed as part of the execution plan for actions in a subsequent cell. This may result in expensive operations being repeatedly executed when developing notebook workflows and working on cells in isolation. You can tell Spark to cache a data frame explicitly to avoid this and speed-up significantly subsequent cell operations</p>\n<pre><code>df = spark.sql(&quot;select column_1, column_2, ... from gaia_source where ...&quot;).cache()\n</code></pre>\n<p>but obviously this should only be done within reason and with due regard to the amount of local memory/disk required by the worker nodes: make sure you only cache what you really need. For further details see <a href=\"https://towardsdatascience.com/best-practices-for-caching-in-spark-sql-b22fb0f02d34\">this useful article on best practice with Spark caching</a>. Some of the example notebooks provided use caching to expedite the execution of cells that re-use data frames.</p>\n<p><strong>Indexing:</strong> The data hosted on this platform are arranged by gaia_source.source_id as the primary unique identifier in Gaia Data Releases. This key should be employed when joining distinct data sets, e.g. pre-computed best neighbours from external survey catalogues - see the &ldquo;Data holdings&rdquo; notebook. It can also be used to pull out specific records quickly:</p>\n<pre><code>spark.sql(&quot;select * from gaia_source where source_id = ...&quot;).show()\n</code></pre>\n<p>Otherwise, the general RDBMS paradigm employing SQL indexes to optimise execution plans of queries predicated on certain columns has no equivalent on here. It is worth noting however that the combination of &ldquo;filter push-down&rdquo; and the underlying storage format (<a href=\"http://parquet.apache.org/documentation/latest/\">Parquet</a>) result in only those data that are needed being read from disk on this patform. This is in contrast to RDBMS where columns are generally read from pages within disk files regardless of need unless covered by an index employed as part of an optimised execution plan. This is one of the ways in which Spark achieves much faster performance in &ldquo;big data&rdquo; analysis.</p>\n<p><strong>Distributed processing model:</strong> Fast performance in handling very large data sets is achieved via cluster parallelism with actions split up and optimised for execution on concurrent workers&hellip; anything to be said here?</p>\n</div>"
          }
        ]
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1639345822509_289019187",
      "id": "20210507-152746_1171284722",
      "dateCreated": "2021-12-12T21:50:22+0000",
      "status": "READY",
      "$$hashKey": "object:34182"
    },
    {
      "title": "Further reading and resources",
      "text": "%md\n\nIn addition to the example notebooks provided here and the linked resources therein see the following for further information:\n\n* ...\n* \n\n",
      "user": "anonymous",
      "dateUpdated": "2021-12-12T21:50:22+0000",
      "progress": 0,
      "config": {
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true,
          "completionKey": "TAB",
          "completionSupport": false
        },
        "colWidth": 12,
        "editorMode": "ace/mode/markdown",
        "fontSize": 9,
        "title": true,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1639345822509_1253553155",
      "id": "20210510-090543_1039599855",
      "dateCreated": "2021-12-12T21:50:22+0000",
      "status": "READY",
      "$$hashKey": "object:34183"
    }
  ],
  "name": "AAA Start here",
  "id": "2GRTQZFUM",
  "defaultInterpreterGroup": "spark",
  "version": "0.10.0",
  "noteParams": {},
  "noteForms": {},
  "angularObjects": {},
  "config": {
    "isZeppelinNotebookCronEnable": false,
    "looknfeel": "default",
    "personalizedMode": "false"
  },
  "info": {},
  "path": "/Public Examples/AAA Start here"
}