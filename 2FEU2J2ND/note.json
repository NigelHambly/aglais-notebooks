{
  "paragraphs": [
    {
      "text": "%spark.pyspark\nsc._jsc.hadoopConfiguration().set(\n    \"fs.s3a.endpoint\", \"cumulus.openstack.hpc.cam.ac.uk:6780/\"\n    )\nsc._jsc.hadoopConfiguration().set(\n    \"fs.s3a.path.style.access\", \"true\"\n    )\nsc._jsc.hadoopConfiguration().set(\n    \"fs.s3a.list.version\", \"2\"\n    )\nsc._jsc.hadoopConfiguration().set(\n    \"fs.s3a.bucket.probe\", \"0\"\n    )\nsc._jsc.hadoopConfiguration().set(\n    \"fs.s3a.aws.credentials.provider\",\n    \"org.apache.hadoop.fs.s3a.AnonymousAWSCredentialsProvider\"\n    )",
      "user": "gaiauser",
      "dateUpdated": "2020-08-11 14:06:59.377",
      "config": {
        "colWidth": 12.0,
        "fontSize": 9.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "python",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "editorMode": "ace/mode/python"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": []
      },
      "apps": [],
      "jobName": "paragraph_1597154768427_-1896629065",
      "id": "20200811-140608_674407636",
      "dateCreated": "2020-08-11 14:06:08.427",
      "dateStarted": "2020-08-11 14:06:59.391",
      "dateFinished": "2020-08-11 14:06:59.415",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%spark.pyspark\ndf \u003d sqlContext.read.parquet(\n    \"s3a://gaia-dr2-parquet/part-00000-70392076-8b82-4457-8828-22069e7626e9-c000.snappy.parquet\"\n    )\n\nprint \"DF count: \", df.count()\nprint \"DF partitions: \", df.rdd.getNumPartitions()\n",
      "user": "gaiauser",
      "dateUpdated": "2020-08-11 14:07:10.413",
      "config": {
        "colWidth": 12.0,
        "fontSize": 9.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "python",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "editorMode": "ace/mode/python"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "ERROR",
        "msg": [
          {
            "type": "TEXT",
            "data": "Py4JJavaError: An error occurred while calling o847.parquet.\n: java.lang.RuntimeException: java.lang.ClassNotFoundException: Class org.apache.hadoop.fs.s3a.S3AFileSystem not found\n\tat org.apache.hadoop.conf.Configuration.getClass(Configuration.java:2195)\n\tat org.apache.hadoop.fs.FileSystem.getFileSystemClass(FileSystem.java:2654)\n\tat org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:2667)\n\tat org.apache.hadoop.fs.FileSystem.access$200(FileSystem.java:94)\n\tat org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:2703)\n\tat org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:2685)\n\tat org.apache.hadoop.fs.FileSystem.get(FileSystem.java:373)\n\tat org.apache.hadoop.fs.Path.getFileSystem(Path.java:295)\n\tat org.apache.spark.sql.execution.datasources.DataSource$$anonfun$org$apache$spark$sql$execution$datasources$DataSource$$checkAndGlobPathIfNecessary$1.apply(DataSource.scala:547)\n\tat org.apache.spark.sql.execution.datasources.DataSource$$anonfun$org$apache$spark$sql$execution$datasources$DataSource$$checkAndGlobPathIfNecessary$1.apply(DataSource.scala:545)\n\tat scala.collection.TraversableLike$$anonfun$flatMap$1.apply(TraversableLike.scala:241)\n\tat scala.collection.TraversableLike$$anonfun$flatMap$1.apply(TraversableLike.scala:241)\n\tat scala.collection.immutable.List.foreach(List.scala:392)\n\tat scala.collection.TraversableLike$class.flatMap(TraversableLike.scala:241)\n\tat scala.collection.immutable.List.flatMap(List.scala:355)\n\tat org.apache.spark.sql.execution.datasources.DataSource.org$apache$spark$sql$execution$datasources$DataSource$$checkAndGlobPathIfNecessary(DataSource.scala:545)\n\tat org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:359)\n\tat org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:223)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:211)\n\tat org.apache.spark.sql.DataFrameReader.parquet(DataFrameReader.scala:644)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: java.lang.ClassNotFoundException: Class org.apache.hadoop.fs.s3a.S3AFileSystem not found\n\tat org.apache.hadoop.conf.Configuration.getClassByName(Configuration.java:2101)\n\tat org.apache.hadoop.conf.Configuration.getClass(Configuration.java:2193)\n\t... 30 more\n\n(\u003cclass \u0027py4j.protocol.Py4JJavaError\u0027\u003e, Py4JJavaError(u\u0027An error occurred while calling o847.parquet.\\n\u0027, JavaObject id\u003do850), \u003ctraceback object at 0x7fb26e4ba4b0\u003e)"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1597154819377_-209697842",
      "id": "20200811-140659_293021007",
      "dateCreated": "2020-08-11 14:06:59.377",
      "dateStarted": "2020-08-11 14:07:10.424",
      "dateFinished": "2020-08-11 14:07:11.037",
      "status": "ERROR",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%spark.pyspark\n",
      "user": "gaiauser",
      "dateUpdated": "2020-08-11 14:07:10.414",
      "config": {},
      "settings": {
        "params": {},
        "forms": {}
      },
      "apps": [],
      "jobName": "paragraph_1597154830413_593706865",
      "id": "20200811-140710_860535928",
      "dateCreated": "2020-08-11 14:07:10.413",
      "status": "READY",
      "progressUpdateIntervalMs": 500
    }
  ],
  "name": "experiments/zrq/ramble",
  "id": "2FEU2J2ND",
  "noteParams": {},
  "noteForms": {},
  "angularObjects": {
    "md:shared_process": [],
    "python:shared_process": [],
    "spark:shared_process": []
  },
  "config": {
    "isZeppelinNotebookCronEnable": false
  },
  "info": {}
}