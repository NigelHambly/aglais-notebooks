{
  "paragraphs": [
    {
      "text": "%md\n\n# Using ML to define an astrometrically clean sample of stars\n\n   Follows Gaia EDR3 performance verification paper DPACP-81 (Smart et al.) in classifying astrometric solutions as good or bad\n   via supervised ML. Employs a Random Forrest classifier plus appropriately defined training sets - see\n\n   https://gaia.esac.esa.int/dpacsvn/DPAC/docs/DpacPublications/DR3PerformanceVerification/DPACP-81/main_submitted2308.pdf\n\n   (DPAC password protected) for further details. The work flow implemented here follows closely that described in Section 2, \"GCNS Generation\"\n   (GCNS \u003d Gaia Catalogue of Nearby Stars) and is designed to clean up a 100pc (\u003d nearby) sample.\n\n   Presently implemented for Gaia DR2; deploy and check this implementation against GEDR3 when released - it should reproduce what\u0027s in the paper.\n\n   \u003ci\u003eVersion employing newer, richer dataframe API in pyspark ML\u003c/i\u003e\n   \n   \u003cb\u003eIMPORTANT NOTE: \u003c/b\u003e current deployment has Spark 2.4.7 installed. That specific version\u0027s API is documented here:\n   \n   https://spark.apache.org/docs/2.4.7/ml-classification-regression.html#random-forest-classifier\n   \n   Beware of following on-line message board and other fora posts for help and examples as they more often than not describe and link to different versions, and the API is evolving \u003ci\u003eall the time\u003c/i\u003e.\n   \n   ",
      "user": "gaiauser",
      "dateUpdated": "2021-01-05 10:22:15.296",
      "config": {
        "tableHide": false,
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true,
          "completionKey": "TAB",
          "completionSupport": false
        },
        "colWidth": 12.0,
        "editorMode": "ace/mode/markdown",
        "fontSize": 9.0,
        "editorHide": true,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "\u003cdiv class\u003d\"markdown-body\"\u003e\n\u003ch1\u003eUsing ML to define an astrometrically clean sample of stars\u003c/h1\u003e\n\u003cp\u003eFollows Gaia EDR3 performance verification paper DPACP-81 (Smart et al.) in classifying astrometric solutions as good or bad\u003cbr/\u003e via supervised ML. Employs a Random Forrest classifier plus appropriately defined training sets - see\u003c/p\u003e\n\u003cp\u003e\u003ca href\u003d\"https://gaia.esac.esa.int/dpacsvn/DPAC/docs/DpacPublications/DR3PerformanceVerification/DPACP-81/main_submitted2308.pdf\"\u003ehttps://gaia.esac.esa.int/dpacsvn/DPAC/docs/DpacPublications/DR3PerformanceVerification/DPACP-81/main_submitted2308.pdf\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003e(DPAC password protected) for further details. The work flow implemented here follows closely that described in Section 2, \u0026ldquo;GCNS Generation\u0026rdquo;\u003cbr/\u003e (GCNS \u003d Gaia Catalogue of Nearby Stars) and is designed to clean up a 100pc (\u003d nearby) sample.\u003c/p\u003e\n\u003cp\u003ePresently implemented for Gaia DR2; deploy and check this implementation against GEDR3 when released - it should reproduce what\u0026rsquo;s in the paper.\u003c/p\u003e\n\u003cp\u003e\u003ci\u003eVersion employing newer, richer dataframe API in pyspark ML\u003c/i\u003e\u003c/p\u003e\n\u003cp\u003e\u003cb\u003eIMPORTANT NOTE: \u003c/b\u003e current deployment has Spark 2.4.7 installed. That specific version\u0026rsquo;s API is documented here:\u003c/p\u003e\n\u003cp\u003e\u003ca href\u003d\"https://spark.apache.org/docs/2.4.7/ml-classification-regression.html#random-forest-classifier\"\u003ehttps://spark.apache.org/docs/2.4.7/ml-classification-regression.html#random-forest-classifier\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003eBeware of following on-line message board and other fora posts for help and examples as they more often than not describe and link to different versions, and the API is evolving \u003ci\u003eall the time\u003c/i\u003e.\u003c/p\u003e\n\u003c/div\u003e"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1606207963366_2005713725",
      "id": "20201013-131059_546082898",
      "dateCreated": "2020-11-24 08:52:43.366",
      "dateStarted": "2021-01-05 10:22:15.266",
      "dateFinished": "2021-01-05 10:22:17.791",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%spark.pyspark\n\n# this is the set of astrometric features to be used. In reality several iterations of this workflow might be required with an expanded set, and some figure-of-merit,\n# e.g. Gini index, would be used to select those most important to the RF classification - cf. Table A.1 in the GCNS paper.\nastrometric_features \u003d [\n    \u0027parallax_error\u0027, \n    \u0027parallax_over_error\u0027,\n    \u0027astrometric_sigma5d_max\u0027,\n    \u0027pmra_error\u0027,\n    \u0027pmdec_error\u0027,\n    \u0027astrometric_excess_noise\u0027,\n    \u0027ipd_gof_harmonic_amplitude\u0027,\n    \u0027ruwe\u0027, \n    \u0027visibility_periods_used\u0027,\n    \u0027pmdec\u0027,\n    \u0027pmra\u0027,\n    \u0027ipd_frac_odd_win\u0027,\n    \u0027ipd_frac_multi_peak\u0027,\n    \u0027astrometric_gof_al\u0027,\n    \u0027scan_direction_strength_k2\u0027,\n    \u0027parallax_pmdec_corr\u0027\n]\n# ... the last two are included to cross check against the Gini index results presented in the paper.\n\n# quick mode: set an additional predicate filter on random_index here to limit to 1% or 0.1% sampling etc:\nquick_filter \u003d \u0027 AND MOD(random_index, 10) \u003d 0\u0027\n# ... to switch this off, simply specify an empty string. But to avoid overloading matplotlib when visualising results, keep this one:\nquick_plot_filter \u003d \u0027 AND MOD(random_index, 10) \u003d 0\u0027\n\n# Default Spark worker configuration cannot atm handle the full dataset - maybe this is (at least part of) the problem:\n# https://stackoverflow.com/questions/25707784/why-does-a-job-fail-with-no-space-left-on-device-but-df-says-otherwise\n\n# reformat the above attribute list into an SQL comma-separated select string\nfeatures_select_string \u003d (\u0027%s, \u0027*(len(astrometric_features) - 1) + \u0027%s \u0027)%tuple(astrometric_features)\n#print (features_select_string)\n\n# Confirmed by Luis Sarro, personal communication: actually we train on ABS(parallax_over_error), see e.g. GCNS paper Figure A.5\nfeatures_select_string \u003d features_select_string.replace(\u0027parallax_over_error\u0027,\u0027ABS(parallax_over_error) AS parallax_over_error\u0027)\n\n# photometric consistency predicate - e.g. Evans et al. (2018), Babusiaux et al. (2018) for DR2:\n#photometric_consistency_filter \u003d \u0027 AND phot_bp_rp_excess_factor BETWEEN 1.0 + (0.03 * POW(bp_rp, 2.0)) AND 1.3 + (0.06 * POW(bp_rp, 2.0))\u0027\n# Riello et al. (2020) for EDR3: fgbp_grp defined by Equation 6 and coefficients in Table 2; sig_cstarg defined by Equation 18:\nphotometric_consistency_filter \u003d \u0027 AND (\u0027 + \\\n    \u0027(bp_rp \u003c 0.5 AND (phot_bp_rp_excess_factor - fgbp_grp_0p5 \u003c 5.0 * sig_cstarg) OR \u0027 + \\\n    \u0027(bp_rp BETWEEN 0.5 AND 0.4 AND (phot_bp_rp_excess_factor - fgbp_grp_0p5_4p0 \u003c 5.0 * sig_cstarg)) OR \u0027 + \\\n    \u0027(bp_rp \u003e\u003d 4.0 AND (phot_bp_rp_excess_factor - fgbp_grp_4p0 \u003c 5.0 * sig_cstarg)))\u0027\nphotometric_consistency_indicators \u003d \\\n    \u00271.15436 + 0.033772*bp_rp + 0.032277*bp_rp*bp_rp AS fgbp_grp_0p5, \u0027 + \\\n    \u00271.162004 + 0.011464*bp_rp + 0.049255*bp_rp*bp_rp -0.005879*bp_rp*bp_rp*bp_rp AS fgbp_grp_0p5_4p0, \u0027 + \\\n    \u00271.057572 + 0.0140537*bp_rp AS fgbp_grp_4p0, \u0027 + \\\n    \u00270.0059898 + 8.817481e-12*POW(phot_g_mean_mag, 7.618399) AS sig_cstarg\u0027\n\n# define the data source TODO change to the \"official\" location as and when:\ngs_df \u003d sqlContext.read.parquet(\u0027file:////user/nch/PARQUET/TESTS/GEDR3/*.parquet\u0027)\n\n# register as SQL-queryable \ngs_df.createOrReplaceTempView(\u0027gaia_source\u0027)\n\n",
      "user": "gaiauser",
      "dateUpdated": "2021-01-05 11:37:19.960",
      "config": {
        "editorSetting": {
          "language": "python",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "colWidth": 12.0,
        "editorMode": "ace/mode/python",
        "fontSize": 9.0,
        "editorHide": false,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "ERROR",
        "msg": [
          {
            "type": "TEXT",
            "data": "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)\n\u001b[0;32m\u003cipython-input-68-8effafe24908\u003e\u001b[0m in \u001b[0;36m\u003cmodule\u003e\u001b[0;34m\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[0;31m# define the data source TODO change to the \"official\" location as and when:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---\u003e 52\u001b[0;31m \u001b[0mgs_df\u001b[0m \u001b[0;34m\u003d\u001b[0m \u001b[0msqlContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparquet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u0027file:////user/nch/PARQUET/TESTS/GEDR3/*.parquet\u0027\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0;31m# register as SQL-queryable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\n\u001b[0;32m/opt/spark/python/lib/pyspark.zip/pyspark/sql/readwriter.py\u001b[0m in \u001b[0;36mparquet\u001b[0;34m(self, *paths)\u001b[0m\n\u001b[1;32m    314\u001b[0m         \u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u0027name\u0027\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\u0027string\u0027\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\u0027year\u0027\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\u0027int\u0027\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\u0027month\u0027\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\u0027int\u0027\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\u0027day\u0027\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\u0027int\u0027\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    315\u001b[0m         \"\"\"\n\u001b[0;32m--\u003e 316\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_df\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jreader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparquet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_to_seq\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_spark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpaths\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    317\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    318\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mignore_unicode_prefix\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\n\u001b[0;32m/opt/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1255\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m\u003d\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1256\u001b[0m         return_value \u003d get_return_value(\n\u001b[0;32m-\u003e 1257\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1259\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\n\u001b[0;32m/opt/spark/python/lib/pyspark.zip/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---\u003e 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0ms\u001b[0m \u001b[0;34m\u003d\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\n\u001b[0;32m/opt/spark/python/lib/py4j-0.10.7-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    326\u001b[0m                 raise Py4JJavaError(\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--\u003e 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m                 raise Py4JError(\n\n\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o549.parquet.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 2554 in stage 67.0 failed 4 times, most recent failure: Lost task 2554.3 in stage 67.0 (TID 135124, worker05, executor 7): java.lang.NoClassDefFoundError: Could not initialize class java.nio.file.FileSystems$DefaultFileSystemHolder\n\tat java.nio.file.FileSystems.getDefault(FileSystems.java:176)\n\tat java.io.File.toPath(File.java:2234)\n\tat org.apache.spark.util.Utils$.copyFile(Utils.scala:632)\n\tat org.apache.spark.util.Utils$.downloadFile(Utils.scala:566)\n\tat org.apache.spark.util.Utils$.doFetchFile(Utils.scala:695)\n\tat org.apache.spark.util.Utils$.fetchFile(Utils.scala:496)\n\tat org.apache.spark.executor.Executor$$anonfun$org$apache$spark$executor$Executor$$updateDependencies$5.apply(Executor.scala:816)\n\tat org.apache.spark.executor.Executor$$anonfun$org$apache$spark$executor$Executor$$updateDependencies$5.apply(Executor.scala:808)\n\tat scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:733)\n\tat scala.collection.mutable.HashMap$$anonfun$foreach$1.apply(HashMap.scala:130)\n\tat scala.collection.mutable.HashMap$$anonfun$foreach$1.apply(HashMap.scala:130)\n\tat scala.collection.mutable.HashTable$class.foreachEntry(HashTable.scala:236)\n\tat scala.collection.mutable.HashMap.foreachEntry(HashMap.scala:40)\n\tat scala.collection.mutable.HashMap.foreach(HashMap.scala:130)\n\tat scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:732)\n\tat org.apache.spark.executor.Executor.org$apache$spark$executor$Executor$$updateDependencies(Executor.scala:808)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:375)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1925)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1913)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1912)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1912)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:948)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:948)\n\tat scala.Option.foreach(Option.scala:257)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:948)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2146)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2095)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2084)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:759)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2061)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2082)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2101)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2126)\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:990)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:385)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:989)\n\tat org.apache.spark.sql.execution.datasources.InMemoryFileIndex$.bulkListLeafFiles(InMemoryFileIndex.scala:241)\n\tat org.apache.spark.sql.execution.datasources.InMemoryFileIndex.listLeafFiles(InMemoryFileIndex.scala:127)\n\tat org.apache.spark.sql.execution.datasources.InMemoryFileIndex.refresh0(InMemoryFileIndex.scala:91)\n\tat org.apache.spark.sql.execution.datasources.InMemoryFileIndex.\u003cinit\u003e(InMemoryFileIndex.scala:67)\n\tat org.apache.spark.sql.execution.datasources.DataSource.org$apache$spark$sql$execution$datasources$DataSource$$createInMemoryFileIndex(DataSource.scala:547)\n\tat org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:385)\n\tat org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:242)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:230)\n\tat org.apache.spark.sql.DataFrameReader.parquet(DataFrameReader.scala:664)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: java.lang.NoClassDefFoundError: Could not initialize class java.nio.file.FileSystems$DefaultFileSystemHolder\n\tat java.nio.file.FileSystems.getDefault(FileSystems.java:176)\n\tat java.io.File.toPath(File.java:2234)\n\tat org.apache.spark.util.Utils$.copyFile(Utils.scala:632)\n\tat org.apache.spark.util.Utils$.downloadFile(Utils.scala:566)\n\tat org.apache.spark.util.Utils$.doFetchFile(Utils.scala:695)\n\tat org.apache.spark.util.Utils$.fetchFile(Utils.scala:496)\n\tat org.apache.spark.executor.Executor$$anonfun$org$apache$spark$executor$Executor$$updateDependencies$5.apply(Executor.scala:816)\n\tat org.apache.spark.executor.Executor$$anonfun$org$apache$spark$executor$Executor$$updateDependencies$5.apply(Executor.scala:808)\n\tat scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:733)\n\tat scala.collection.mutable.HashMap$$anonfun$foreach$1.apply(HashMap.scala:130)\n\tat scala.collection.mutable.HashMap$$anonfun$foreach$1.apply(HashMap.scala:130)\n\tat scala.collection.mutable.HashTable$class.foreachEntry(HashTable.scala:236)\n\tat scala.collection.mutable.HashMap.foreachEntry(HashMap.scala:40)\n\tat scala.collection.mutable.HashMap.foreach(HashMap.scala:130)\n\tat scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:732)\n\tat org.apache.spark.executor.Executor.org$apache$spark$executor$Executor$$updateDependencies(Executor.scala:808)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:375)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\n"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1606207963368_-1981729652",
      "id": "20201013-131649_1734629667",
      "dateCreated": "2020-11-24 08:52:43.368",
      "dateStarted": "2021-01-05 11:33:34.363",
      "dateFinished": "2021-01-05 11:34:26.974",
      "status": "ERROR",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%spark.pyspark\n\n# clear any previously cached data in the context (cells may be executed in any order, and out-dated by changes from here onwards)\nsqlContext.clearCache()\n\n# a conservative selection of everything that COULD be within 100pc, including things with measured \n# distances putting them outside the 100pc horizon when their true distances are within, and also including \n# loads of spurious chaff with the wheat of course, plus bad things with significant, unphysical parallaxes:\nraw_sources_df \u003d spark.sql(\u0027SELECT source_id, random_index, phot_g_mean_mag, phot_bp_rp_excess_factor, bp_rp, g_rp, parallax, ra, dec, b, \u0027 + features_select_string + photometric_consistency_indicators + \u0027FROM gaia_source WHERE ABS(parallax) \u003e 8.0\u0027)\n\n# cache it for speedy access below (all subsequent samples are derived from this):\nraw_sources_df.cache()\n\n# register as SQL-queryable\nraw_sources_df.createOrReplaceTempView(\u0027raw_sources\u0027)\n\nraw_sources_df.count()\n# (cf. GCNS: 1,211,740 sources) ",
      "user": "gaiauser",
      "dateUpdated": "2021-01-05 11:31:01.149",
      "config": {
        "editorSetting": {
          "language": "python",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "colWidth": 12.0,
        "editorMode": "ace/mode/python",
        "fontSize": 9.0,
        "editorHide": false,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "apps": [],
      "jobName": "paragraph_1606207963370_2129974015",
      "id": "20201013-132418_278702125",
      "dateCreated": "2020-11-24 08:52:43.370",
      "dateStarted": "2020-11-25 16:57:33.389",
      "dateFinished": "2020-11-25 16:59:10.856",
      "status": "FINISHED",
      "errorMessage": "",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%spark.pyspark\n\n# plot an observational Hertzsprung-Russell diagram (aka colour / absolute magnitude diagram) for the unclassified sample to show the problem,\n# include the photometric consistency filter to show the problem is astrometric in addition to photometric\nunclassified_camd_df \u003d spark.sql(\u0027SELECT phot_g_mean_mag + 5.0*LOG10(parallax/100.0) AS m_g, g_rp FROM raw_sources WHERE parallax \u003e +8.0\u0027 + quick_plot_filter)# + photometric_consistency_filter)\n\nimport matplotlib.pyplot as plot\nplot.figure(0, figsize \u003d (6.0, 9.7))\nx \u003d list(unclassified_camd_df.select(\u0027g_rp\u0027).toPandas()[\u0027g_rp\u0027])\ny \u003d list(unclassified_camd_df.select(\u0027m_g\u0027).toPandas()[\u0027m_g\u0027])\nplot.scatter(x, y, marker \u003d \u0027.\u0027, s \u003d 1)\nplot.ylim(21.0, -3.0)\nplot.ylabel(\u0027Stellar brightness (absolute G magnitude) --\u003e\u0027, fontsize \u003d 16)\nplot.xlabel(\u0027\u003c-- Stellar temperature (G - RP magnitude)\u0027, fontsize \u003d 16)\nplot.show()\n\n",
      "user": "gaiauser",
      "dateUpdated": "2021-01-05 10:54:11.423",
      "config": {
        "editorSetting": {
          "language": "python",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "colWidth": 12.0,
        "editorMode": "ace/mode/python",
        "fontSize": 9.0,
        "editorHide": true,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "apps": [],
      "jobName": "paragraph_1606207963370_-937914806",
      "id": "20201120-094650_221463065",
      "dateCreated": "2020-11-24 08:52:43.370",
      "dateStarted": "2020-11-25 16:59:31.049",
      "dateFinished": "2020-11-25 17:05:03.391",
      "status": "FINISHED",
      "errorMessage": "",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%md\n\nThe problem: while we see astrophysically interesting locii in this diagram, the lower right (cool, low temperature) regime \u003cbr\u003e is dominated by systematic errors (not random uncertainties - the data should be equally precise in all parts of this data \u003cbr\u003e space) that contaminate the raw sample. We wish to clean the sample to obtain high reliability\n\n* without compromising completeness;\n* utilising astrometric quality features in the raw catalogue for a volume-complete sample;\n* and efficiently; \n\ni.e. without endless iterations of manual, subjective, axis-parallel and arbitrary cuts on available catalogue attributes. A neat solution to this is to use supervised ML. In the Gaia EDR3 performance verification paper \"Gaia Catalogue of Nearby Stars\" (Smart, Sarro, Rybicki, et al. 2020) we use a Random Forest of decision trees on selected features having first defined a training set based on the data itself. \n\nNote that plotting the \u003ci\u003eintrinsic brightness\u003c/i\u003e of a star as above requires determination of the \u003ci\u003edistance\u003c/i\u003e to it along with a measurement of it\u0027s apparent brightness. Stellar distance determination is a fundamental goal of the Gaia mission and is achieved via measurement of the \u003ci\u003estellar parallax\u003c/i\u003e, the apparent \"wobble\" in angular position exhibited by all stars as seen from our (annually changing) view point in the solar system. The sample plotted above is selected for parallax \u003e 8 milliarcseconds (mas) which corresponds to a distance within 125 parsecs (since a star at distance 1 parsec exhibits a parallax of 1 arcsecond; parsec \u003d \"parallax arcsecond\"; 1 parsec is around 3.3 light-years).\n\nAn 8 mas training set of \"good\" examples is \"cleaned\" of highly probable spurious sources using \u003ci\u003eindependent\u003c/i\u003e photometric criteria (i.e. we require consistency of optical and infrared colours). The \"bad\" examples are selected having (unphysical) parallax \u003c -8 mas, i.e. using parallax measurements that are formally highly significant, yet obviously spurious. Under the assumption of normally distributed uncertainties on the parallax measurements, this bad sample should be representative of the corresponding spurious measurements having parallax \u003e 8 mas that contaminate the parallax-selected sample and, in particular, create the contamination illustrated in the plot above.\n",
      "user": "gaiauser",
      "dateUpdated": "2021-01-05 10:54:19.928",
      "config": {
        "tableHide": false,
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true,
          "completionKey": "TAB",
          "completionSupport": false
        },
        "colWidth": 12.0,
        "editorMode": "ace/mode/markdown",
        "fontSize": 9.0,
        "editorHide": true,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "apps": [],
      "jobName": "paragraph_1606207963372_1994972809",
      "id": "20201120-110502_1704727157",
      "dateCreated": "2020-11-24 08:52:43.372",
      "status": "READY",
      "errorMessage": "",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%spark.pyspark\n\n# good training data: first define rough positional cuts to exclude crowded regions at low Gaialctic latitude, and inside the Large and Small Magellanic Clouds (Luri et al. 2020):\nlow_galactic_latitude_filter \u003d \u0027 AND ABS(b) \u003e 25.0\u0027\nsmc_filter \u003d \u0027 AND (dec \u003c -80.0 OR dec \u003e -65.0 OR (ra \u003c 350.0 AND ra \u003e +40.0))\u0027\nlmc_filter \u003d \u0027 AND (dec \u003c -80.0 OR dec \u003e -55.0 OR ra \u003c 40.0 OR ra \u003e 120.0)\u0027\nall_good_training_df \u003d spark.sql(\u0027SELECT 1 AS label, \u0027 + features_select_string + \u0027 FROM raw_sources WHERE parallax \u003e + 8.0 AND ABS(b) \u003e 25.0\u0027 + photometric_consistency_filter + quick_filter + low_galactic_latitude_filter + smc_filter + lmc_filter)\ngood_training_rows \u003d all_good_training_df.count()\n#print(\u0027Good training data size: %d rows\u0027%(good_training_rows))\n\n# bad training data: negative parallaxes: N.B. make a selection exactly the same size as the good training set based on size of smaller (good) data set and count of all available bads\nmaximal_bad_ast_count \u003d spark.sql(\u0027SELECT source_id FROM raw_sources WHERE parallax \u003c -8.0\u0027).count()\nfilter_factor \u003d int(maximal_bad_ast_count / good_training_rows)\nall_bad_training_df \u003d spark.sql(\u0027SELECT 0 AS label, \u0027 + features_select_string + \u0027 FROM raw_sources WHERE  parallax \u003c -8.0 AND MOD(random_index, %d) \u003d 0\u0027%(filter_factor) + \u0027 ORDER BY random_index LIMIT %d\u0027%(good_training_rows))\n#all_bad_training_data_count \u003d all_bad_training_df.count()\n#print (\u0027Bad  training data size: %d rows\u0027%(all_bad_training_data_count))\n",
      "user": "gaiauser",
      "dateUpdated": "2020-12-07 16:59:31.904",
      "config": {
        "editorSetting": {
          "language": "python",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "colWidth": 12.0,
        "editorMode": "ace/mode/python",
        "fontSize": 9.0,
        "results": {},
        "enabled": true,
        "editorHide": false
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "apps": [],
      "jobName": "paragraph_1606207963373_98475139",
      "id": "20201123-105445_95907042",
      "dateCreated": "2020-11-24 08:52:43.373",
      "dateStarted": "2020-11-25 17:05:35.900",
      "dateFinished": "2020-11-25 17:10:52.721",
      "status": "FINISHED",
      "errorMessage": "",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%spark.pyspark\n\n# define training (67%) and test (33%) sample splits (seeded randomness for repeatability)\ngood_67pc, good_33pc \u003d all_good_training_df.randomSplit([0.67, 0.33], 42)\nbad_67pc, bad_33pc \u003d all_bad_training_df.randomSplit([0.67, 0.33], 42)\n\n# transform to labelled feature vectors (0.0 \u003d bad, 1.0 \u003d good, as conveniently already defined in previous projections above)\n\n# Annotate and transform appropriate to the input required by the classifier\u0027s API.\n# Need a dataframe with labels and features: use vector assembler. \nfrom pyspark.ml.feature import VectorAssembler\nignore \u003d [\u0027label\u0027,]\nassembler \u003d VectorAssembler(inputCols\u003d[x for x in good_67pc.columns if x not in ignore], outputCol\u003d\u0027features\u0027)\n\n# training sets\ngood_training_df \u003d assembler.transform(good_67pc).drop(*astrometric_features)\nbad_training_df \u003d assembler.transform(bad_67pc).drop(*astrometric_features)\n# ... N.B. the original individual feature columns are dropped to save memory (since they are duplicated into the resulting feature vector).\n\n# testing sets\ngood_testing_df \u003d assembler.transform(good_33pc).drop(*astrometric_features)\nbad_testing_df \u003d assembler.transform(bad_33pc).drop(*astrometric_features)\n\n# concatenate the training set into a single dataframe\ntraining_df \u003d good_training_df.union(bad_training_df)\n#training_df.show()\n",
      "user": "gaiauser",
      "dateUpdated": "2020-12-07 16:59:29.323",
      "config": {
        "editorSetting": {
          "language": "python",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "colWidth": 12.0,
        "editorMode": "ace/mode/python",
        "fontSize": 9.0,
        "results": {},
        "enabled": true,
        "editorHide": false
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "apps": [],
      "jobName": "paragraph_1606207963375_1659740506",
      "id": "20201015-161110_18118893",
      "dateCreated": "2020-11-24 08:52:43.375",
      "dateStarted": "2020-11-25 17:11:43.003",
      "dateFinished": "2020-11-25 17:11:43.100",
      "status": "FINISHED",
      "errorMessage": "",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%spark.pyspark\n\n# This cell does the business, given the data and training sets. Follows the example Python code at \n# https://spark.apache.org/docs/2.4.7/api/python/pyspark.ml.html#pyspark.ml.classification.RandomForestClassifier\n\nfrom pyspark.ml.classification import RandomForestClassifier\n\n# instantiate a trained RF classifier, seeded for repeatability at this stage:\nrf \u003d RandomForestClassifier(featureSubsetStrategy \u003d \u0027sqrt\u0027, featuresCol \u003d \u0027features\u0027, labelCol \u003d \u0027label\u0027, numTrees \u003d 5000, impurity \u003d \u0027gini\u0027, seed\u003d42)\nmodel \u003d rf.fit(training_df)\n\n# benchmarks: featureSubsetStrategy \u003d \"sqrt\"\n# 10% sample,  100 trees:  3min 58sec (Tues) 7min 57sec (Wed) ...!\n",
      "user": "gaiauser",
      "dateUpdated": "2020-11-25 17:12:12.044",
      "config": {
        "editorSetting": {
          "language": "python",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "colWidth": 12.0,
        "editorMode": "ace/mode/python",
        "fontSize": 9.0,
        "results": {},
        "enabled": true,
        "editorHide": false
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "apps": [],
      "jobName": "paragraph_1606207963376_-1848568621",
      "id": "20201013-152110_1282917873",
      "dateCreated": "2020-11-24 08:52:43.376",
      "dateStarted": "2020-11-25 17:12:12.071",
      "dateFinished": "2020-11-25 18:01:29.403",
      "status": "ERROR",
      "errorMessage": "",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%spark.pyspark\n\n# classify based on the above trained model\ngood_test_results \u003d model.transform(good_testing_df)\nbad_test_results \u003d model.transform(bad_testing_df)\n\n#good_test_results.show()\n\n\n",
      "user": "gaiauser",
      "dateUpdated": "2020-11-25 16:37:52.029",
      "config": {
        "editorSetting": {
          "language": "python",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "colWidth": 12.0,
        "editorMode": "ace/mode/python",
        "fontSize": 9.0,
        "results": {},
        "enabled": true,
        "editorHide": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "apps": [],
      "jobName": "paragraph_1606207963376_-1116057798",
      "id": "20201015-131823_1744793710",
      "dateCreated": "2020-11-24 08:52:43.376",
      "dateStarted": "2020-11-25 14:47:11.929",
      "dateFinished": "2020-11-25 14:47:12.022",
      "status": "FINISHED",
      "errorMessage": "",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%spark.pyspark\n\n# test results numerical output\n\n# count up\nfrom collections import Counter\npositives \u003d Counter(list(good_test_results.select(\u0027prediction\u0027).toPandas()[\u0027prediction\u0027]))\nnegatives \u003d Counter(list(bad_test_results.select(\u0027prediction\u0027).toPandas()[\u0027prediction\u0027]))\n\n# Confusion matrix (after GCNS paper, Table 1):\ntrue_positives \u003d positives[1.0]\nfalse_positives \u003d positives[0.0]\ntrue_negatives \u003d negatives[0.0]\nfalse_negatives \u003d negatives[1.0]\nprint(\u0027   |%7d%7d\u0027%(1,2))\nprint(\u0027------------------------------\u0027)\nprint(\u0027 1 |%7d%7d\u0027%(true_positives, false_positives))\nprint(\u0027 2 |%7d%7d\u0027%(false_negatives, true_negatives))\nprint()\n\n# Misclassification fraction: cf. GCNS paper which quotes 0.1%\nnum_misclassified \u003d false_positives + false_negatives\ntotal_num_in_test \u003d true_positives + true_negatives + num_misclassified\nmisclassified_pc \u003d 100.0 * float(num_misclassified) / float(total_num_in_test)\nprint(\u0027Misclassifications for the test set: %.2f %%\u0027%(misclassified_pc))\n",
      "user": "gaiauser",
      "dateUpdated": "2020-11-25 14:48:44.930",
      "config": {
        "editorSetting": {
          "language": "python",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "colWidth": 12.0,
        "editorMode": "ace/mode/python",
        "fontSize": 9.0,
        "results": {},
        "enabled": true,
        "editorHide": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "apps": [],
      "jobName": "paragraph_1606207963376_1479126773",
      "id": "20201016-154755_24366630",
      "dateCreated": "2020-11-24 08:52:43.376",
      "dateStarted": "2020-11-25 14:47:31.893",
      "dateFinished": "2020-11-25 14:47:44.959",
      "status": "FINISHED",
      "errorMessage": "",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%spark.pyspark\n\n# examine relative importance of features wrt Appendix A.1 of the GCNS paper\nfeature_relative_importance \u003d model.featureImportances.toArray()\nprint(\u0027Relative importance of astrometric features:\\n\u0027)\nfor idx in range(len(astrometric_features)): print(\u0027%25s  :  %f\u0027%(astrometric_features[idx], feature_relative_importance[idx]))\n",
      "user": "gaiauser",
      "dateUpdated": "2020-11-25 14:49:03.642",
      "config": {
        "editorSetting": {
          "language": "python",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "colWidth": 12.0,
        "editorMode": "ace/mode/python",
        "fontSize": 9.0,
        "results": {},
        "enabled": true,
        "editorHide": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "apps": [],
      "jobName": "paragraph_1606207963377_-954351192",
      "id": "20201123-163421_1811049882",
      "dateCreated": "2020-11-24 08:52:43.377",
      "dateStarted": "2020-11-25 14:49:03.664",
      "dateFinished": "2020-11-25 14:49:03.730",
      "status": "FINISHED",
      "errorMessage": "",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%spark.pyspark\n\n# cleaned up CAMD (observational HRD) employing the classifications\n\n# get the complete unclassified sample:\nunclassified_sample_df \u003d spark.sql(\u0027SELECT * FROM raw_sources WHERE parallax \u003e +8.0\u0027 + quick_filter)\n\n# required features subset for the classification model\nassembler \u003d VectorAssembler(inputCols\u003d[x for x in unclassified_sample_df.columns if x in astrometric_features], outputCol\u003d\u0027features\u0027)\ndf_to_classify \u003d assembler.transform(unclassified_sample_df)\nall_classifications \u003d model.transform(df_to_classify)\n#all_classifications.show()\n\n# register as SQL-queryable:\nall_classifications.createOrReplaceTempView(\u0027classified_sources\u0027)\n\n# select on binary classification for a quick check:\ngood_sources_df \u003d spark.sql(\u0027SELECT phot_g_mean_mag + 5.0*LOG10(parallax/100.0) AS m_g, g_rp, ra, dec FROM classified_sources WHERE prediction\u003d1.0\u0027 + quick_plot_filter)# + photometric_consistency_filter)\nbad_sources_df \u003d  spark.sql(\u0027SELECT phot_g_mean_mag + 5.0*LOG10(parallax/100.0) AS m_g, g_rp, ra, dec FROM classified_sources WHERE prediction\u003d0.0\u0027 + quick_plot_filter)# + photometric_consistency_filter)\n\nimport matplotlib.pyplot as plot\nplot.figure(1, figsize \u003d (6.0, 9.7))\nx \u003d list(bad_sources_df.select(\u0027g_rp\u0027).toPandas()[\u0027g_rp\u0027])\ny \u003d list(bad_sources_df.select(\u0027m_g\u0027).toPandas()[\u0027m_g\u0027])\nplot.scatter(x, y, marker \u003d \u0027.\u0027, s \u003d 1, c \u003d \u0027orange\u0027)\nx \u003d list(good_sources_df.select(\u0027g_rp\u0027).toPandas()[\u0027g_rp\u0027])\ny \u003d list(good_sources_df.select(\u0027m_g\u0027).toPandas()[\u0027m_g\u0027])\nplot.scatter(x, y, marker \u003d \u0027.\u0027, s \u003d 1)\nplot.ylim(21.0, -3.0)\nplot.ylabel(\u0027Stellar brightness (absolute G magnitude) --\u003e\u0027, fontsize \u003d 16)\nplot.xlabel(\u0027\u003c-- Stellar temperature (G - RP magnitude)\u0027, fontsize \u003d 16)\nplot.show()\n\n\n",
      "user": "gaiauser",
      "dateUpdated": "2020-11-25 16:36:30.618",
      "config": {
        "editorSetting": {
          "language": "python",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "colWidth": 12.0,
        "editorMode": "ace/mode/python",
        "fontSize": 9.0,
        "results": {},
        "enabled": true,
        "editorHide": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "apps": [],
      "jobName": "paragraph_1606207963377_-2090513373",
      "id": "20201123-162249_1468741293",
      "dateCreated": "2020-11-24 08:52:43.377",
      "dateStarted": "2020-11-25 14:49:40.123",
      "dateFinished": "2020-11-25 14:50:13.890",
      "status": "FINISHED",
      "errorMessage": "",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%spark.pyspark\n\n# histogram of the classification probabilities: cf. GCNS paper Figure 3\n\n#all_classifications.show()\n\nimport matplotlib.pyplot as plot\nplot.figure(1, figsize \u003d (9.7, 6.0))\nplot.yscale(\u0027log\u0027)\nx \u003d list(all_classifications.select(\u0027probability\u0027).toPandas()[\u0027probability\u0027])\nplot.hist(x, bins\u003d25, color\u003d\u0027black\u0027)\nplot.xlabel(\u0027Random Forest Probability\u0027)\nplot.ylabel(\u0027N\u0027)\n\n",
      "user": "gaiauser",
      "dateUpdated": "2020-12-15 10:33:36.085",
      "config": {
        "colWidth": 12.0,
        "fontSize": 9.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "python",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "editorMode": "ace/mode/python",
        "editorHide": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "apps": [],
      "jobName": "paragraph_1606212312380_2125797250",
      "id": "20201124-100512_110153564",
      "dateCreated": "2020-11-24 10:05:12.381",
      "dateStarted": "2020-11-25 14:50:41.732",
      "dateFinished": "2020-11-25 14:50:53.581",
      "status": "FINISHED",
      "errorMessage": "",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%spark.pyspark\n\n# cf. GCNS paper Figure 1 panels: sky distribution of good/bad sources:\n\nimport math\n\nplot.figure(2, figsize \u003d (16.18, 10.0))\nplot.subplot(111, projection\u003d\u0027aitoff\u0027)\nplot.grid(True)\nx \u003d list((good_sources_df.select(\u0027ra\u0027).toPandas()[\u0027ra\u0027] - 180.0) * math.pi / 180.0)\ny \u003d list(good_sources_df.select(\u0027dec\u0027).toPandas()[\u0027dec\u0027] * math.pi / 180.0)\nplot.title(\u0027Good sources\u0027)\nplot.scatter(x, y, marker \u003d \u0027.\u0027, s \u003d 1)\n",
      "user": "gaiauser",
      "dateUpdated": "2020-12-15 10:33:34.749",
      "config": {
        "colWidth": 12.0,
        "fontSize": 9.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "python",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "editorMode": "ace/mode/python",
        "editorHide": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "apps": [],
      "jobName": "paragraph_1606300246694_657398034",
      "id": "20201125-103046_1353183691",
      "dateCreated": "2020-11-25 10:30:46.695",
      "dateStarted": "2020-11-25 16:34:03.733",
      "dateFinished": "2020-11-25 16:34:20.349",
      "status": "FINISHED",
      "errorMessage": "",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%spark.pyspark\n\nplot.figure(3, figsize \u003d (16.18, 10.0))\nplot.subplot(111, projection\u003d\u0027aitoff\u0027)\nplot.grid(True)\nx \u003d list((bad_sources_df.select(\u0027ra\u0027).toPandas()[\u0027ra\u0027] - 180.0) * math.pi / 180.0)\ny \u003d list(bad_sources_df.select(\u0027dec\u0027).toPandas()[\u0027dec\u0027] * math.pi / 180.0)\nplot.title(\u0027Bad sources\u0027)\nplot.scatter(x, y, marker \u003d \u0027.\u0027, s \u003d 1)\n",
      "user": "gaiauser",
      "dateUpdated": "2020-12-18 14:56:58.011",
      "config": {
        "colWidth": 12.0,
        "fontSize": 9.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "python",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "editorMode": "ace/mode/python",
        "editorHide": false
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "apps": [],
      "jobName": "paragraph_1606321992863_2117699663",
      "id": "20201125-163312_728555601",
      "dateCreated": "2020-11-25 16:33:12.863",
      "dateStarted": "2020-11-25 16:34:45.494",
      "dateFinished": "2020-11-25 16:35:02.650",
      "status": "FINISHED",
      "errorMessage": "",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%spark.pyspark\nprint(\"No. of good sources: \",good_sources_df.count())\nprint(\"No. of bad sources:  \",bad_sources_df.count())\n",
      "user": "gaiauser",
      "dateUpdated": "2020-12-18 14:56:56.854",
      "config": {
        "colWidth": 12.0,
        "fontSize": 9.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "python",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "editorMode": "ace/mode/python",
        "editorHide": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "apps": [],
      "jobName": "paragraph_1606319491215_1853015224",
      "id": "20201125-155131_269531128",
      "dateCreated": "2020-11-25 15:51:31.216",
      "dateStarted": "2020-11-25 15:53:44.122",
      "dateFinished": "2020-11-25 15:54:03.807",
      "status": "FINISHED",
      "errorMessage": "",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%spark.pyspark\n\n# histogram of distribution of parallax_over_error\nimport matplotlib.pyplot as plot\nplot.figure(1, figsize \u003d (9.7, 6.0))\nplot.yscale(\"log\")\nx \u003d list(good_67pc.select(\"parallax_over_error\").toPandas()[\"parallax_over_error\"])\ny \u003d list(bad_67pc.select(\"parallax_over_error\").toPandas()[\"parallax_over_error\"])\nplot.hist(x, bins\u003d25, label\u003d\u0027good\u0027)\nplot.hist(y, bins\u003d25, label\u003d\u0027bad\u0027)\nplot.xlabel(\"parallax_over_error\")\nplot.ylabel(\"Frequency\")\nplot.legend(loc\u003d\u0027upper right\u0027)",
      "user": "gaiauser",
      "dateUpdated": "2020-12-18 14:56:54.713",
      "config": {
        "colWidth": 12.0,
        "fontSize": 9.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "python",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "editorMode": "ace/mode/python",
        "editorHide": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "apps": [],
      "jobName": "paragraph_1606234305041_148538781",
      "id": "20201124-161145_1933006801",
      "dateCreated": "2020-11-24 16:11:45.041",
      "dateStarted": "2020-11-24 17:23:02.956",
      "dateFinished": "2020-11-24 17:23:11.613",
      "status": "FINISHED",
      "errorMessage": "",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%spark.pyspark\n",
      "user": "gaiauser",
      "dateUpdated": "2020-11-24 17:13:24.089",
      "config": {},
      "settings": {
        "params": {},
        "forms": {}
      },
      "apps": [],
      "jobName": "paragraph_1606238004088_279215841",
      "id": "20201124-171324_1960205489",
      "dateCreated": "2020-11-24 17:13:24.088",
      "status": "READY",
      "errorMessage": "",
      "progressUpdateIntervalMs": 500
    }
  ],
  "name": "Good astrometric solutions via ML Random Forrest classifier",
  "id": "2FRPC4BFS",
  "noteParams": {},
  "noteForms": {},
  "angularObjects": {
    "md:shared_process": [],
    "python:shared_process": [],
    "spark:shared_process": []
  },
  "config": {
    "isZeppelinNotebookCronEnable": false
  },
  "info": {}
}