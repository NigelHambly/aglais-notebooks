{
  "paragraphs": [
    {
      "title": "Set HEALPix resolution",
      "text": "%pyspark\n\n# set the required HEALPixelisation level here:\nhealpix_level \u003d 6\n# HEALPix level : no. of pixels\n# 4 : 3072\n# 5 : 12288\n# 6 : 49152 ~ 1 square degree pixels\n# 7 : 196608\n",
      "user": "gaiauser",
      "dateUpdated": "2021-08-04 22:09:14.435",
      "config": {
        "colWidth": 12.0,
        "fontSize": 9.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "python",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "editorMode": "ace/mode/python",
        "title": true,
        "editorHide": false
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "ERROR",
        "msg": [
          {
            "type": "TEXT",
            "data": "java.net.ConnectException: Connection refused\n\tat sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)\n\tat sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:714)\n\tat org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)\n\tat org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)\n\tat org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)\n\tat org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:614)\n\tat org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:712)\n\tat org.apache.hadoop.ipc.Client$Connection.access$2900(Client.java:375)\n\tat org.apache.hadoop.ipc.Client.getConnection(Client.java:1528)\n\tat org.apache.hadoop.ipc.Client.call(Client.java:1451)\n\tat org.apache.hadoop.ipc.Client.call(Client.java:1412)\n\tat org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)\n\tat com.sun.proxy.$Proxy13.getClusterMetrics(Unknown Source)\n\tat org.apache.hadoop.yarn.api.impl.pb.client.ApplicationClientProtocolPBClientImpl.getClusterMetrics(ApplicationClientProtocolPBClientImpl.java:206)\n\tat sun.reflect.GeneratedMethodAccessor9.invoke(Unknown Source)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:191)\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)\n\tat com.sun.proxy.$Proxy14.getClusterMetrics(Unknown Source)\n\tat org.apache.hadoop.yarn.client.api.impl.YarnClientImpl.getYarnClusterMetrics(YarnClientImpl.java:487)\n\tat org.apache.spark.deploy.yarn.Client$$anonfun$submitApplication$1.apply(Client.scala:165)\n\tat org.apache.spark.deploy.yarn.Client$$anonfun$submitApplication$1.apply(Client.scala:165)\n\tat org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)\n\tat org.apache.spark.deploy.yarn.Client.logInfo(Client.scala:60)\n\tat org.apache.spark.deploy.yarn.Client.submitApplication(Client.scala:164)\n\tat org.apache.spark.scheduler.cluster.YarnClientSchedulerBackend.start(YarnClientSchedulerBackend.scala:57)\n\tat org.apache.spark.scheduler.TaskSchedulerImpl.start(TaskSchedulerImpl.scala:188)\n\tat org.apache.spark.SparkContext.\u003cinit\u003e(SparkContext.scala:501)\n\tat org.apache.spark.SparkContext$.getOrCreate(SparkContext.scala:2520)\n\tat org.apache.spark.sql.SparkSession$Builder$$anonfun$7.apply(SparkSession.scala:930)\n\tat org.apache.spark.sql.SparkSession$Builder$$anonfun$7.apply(SparkSession.scala:921)\n\tat scala.Option.getOrElse(Option.scala:121)\n\tat org.apache.spark.sql.SparkSession$Builder.getOrCreate(SparkSession.scala:921)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat org.apache.zeppelin.spark.BaseSparkScalaInterpreter.spark2CreateContext(BaseSparkScalaInterpreter.scala:263)\n\tat org.apache.zeppelin.spark.BaseSparkScalaInterpreter.createSparkContext(BaseSparkScalaInterpreter.scala:182)\n\tat org.apache.zeppelin.spark.SparkScala211Interpreter.open(SparkScala211Interpreter.scala:90)\n\tat org.apache.zeppelin.spark.NewSparkInterpreter.open(NewSparkInterpreter.java:102)\n\tat org.apache.zeppelin.spark.SparkInterpreter.open(SparkInterpreter.java:62)\n\tat org.apache.zeppelin.interpreter.LazyOpenInterpreter.open(LazyOpenInterpreter.java:69)\n\tat org.apache.zeppelin.spark.PySparkInterpreter.getSparkInterpreter(PySparkInterpreter.java:664)\n\tat org.apache.zeppelin.spark.PySparkInterpreter.createGatewayServerAndStartScript(PySparkInterpreter.java:260)\n\tat org.apache.zeppelin.spark.PySparkInterpreter.open(PySparkInterpreter.java:194)\n\tat org.apache.zeppelin.interpreter.LazyOpenInterpreter.open(LazyOpenInterpreter.java:69)\n\tat org.apache.zeppelin.interpreter.remote.RemoteInterpreterServer$InterpretJob.jobRun(RemoteInterpreterServer.java:616)\n\tat org.apache.zeppelin.scheduler.Job.run(Job.java:188)\n\tat org.apache.zeppelin.scheduler.FIFOScheduler$1.run(FIFOScheduler.java:140)\n\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n\tat java.util.concurrent.FutureTask.run(FutureTask.java:266)\n\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$201(ScheduledThreadPoolExecutor.java:180)\n\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:293)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\n"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1620645476558_-1214107099",
      "id": "20210510-111756_391695716",
      "dateCreated": "2021-05-10 11:17:56.558",
      "dateStarted": "2021-08-04 22:09:14.448",
      "dateFinished": "2021-08-04 22:49:43.609",
      "status": "ERROR",
      "progressUpdateIntervalMs": 500
    },
    {
      "title": "Define a data frame by SQL query",
      "text": "%pyspark\nimport math\n\n# compute relevant pixelisation quantities\nnside \u003d int(math.pow(2, healpix_level))\npowers_of_2 \u003d 35 + (12 - healpix_level)*2\ndivisor \u003d int(math.pow(2, powers_of_2))\n\n# formulate SQL query\nquery \u003d \"SELECT floor(source_id /  %d\"%(divisor) + \") AS hpx_id, COUNT(*) AS n, AVG(pmra) AS avg_pmra, AVG(pmdec) AS avg_pmdec FROM gaia_source GROUP BY hpx_id\"\n\n# define a data frame aggregation of the relevant quantities (note this is cached for use in two subsequent cells)\ndf \u003d spark.sql(query).cache()\n",
      "user": "zrq",
      "dateUpdated": "2021-07-31 04:29:08.067",
      "config": {
        "colWidth": 12.0,
        "fontSize": 9.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "python",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "editorMode": "ace/mode/python",
        "title": true,
        "editorHide": false
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "apps": [],
      "jobName": "paragraph_1620645338466_-1902611713",
      "id": "20210510-111538_106023214",
      "dateCreated": "2021-05-10 11:15:38.466",
      "dateStarted": "2021-07-31 04:29:08.124",
      "dateFinished": "2021-07-31 04:29:10.278",
      "status": "FINISHED",
      "errorMessage": "",
      "progressUpdateIntervalMs": 500
    },
    {
      "title": "Mean RA proper motion plot",
      "text": "%pyspark\n\n# plot up the sky counts\nimport matplotlib.pyplot as plot\nimport numpy as np\nimport healpy as hp\n\n# set a figure to use along with a plot size (landscape, golden ratio)\nplot.figure(1, figsize \u003d (16.18, 10.0))\n\n# healpy constants appropriate to the HEALPix indexing encoded in Gaia source IDs\nnpix \u003d hp.nside2npix(nside)\n\n# do the visualisation\narray_data \u003d np.empty(npix)\nfor item in df.rdd.collect():  array_data[item[0]] \u003d item[2]\nhp.mollview(array_data, fig \u003d 1, coord\u003d\u0027C\u0027, unit\u003d\u0027mas/yr\u0027, nest\u003dTrue, title\u003d\u0027Mean RA proper motion at HEALPix level %d\u0027%(healpix_level), cmap\u003d\u0027rainbow\u0027)\nhp.graticule(coord\u003d\u0027C\u0027, color\u003d\u0027white\u0027)",
      "user": "zrq",
      "dateUpdated": "2021-07-31 04:29:10.326",
      "config": {
        "colWidth": 12.0,
        "fontSize": 9.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "python",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "editorMode": "ace/mode/python",
        "title": true,
        "editorHide": false
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "apps": [],
      "jobName": "paragraph_1620645579333_-397450822",
      "id": "20210510-111939_1386609632",
      "dateCreated": "2021-05-10 11:19:39.333",
      "dateStarted": "2021-07-31 04:29:10.629",
      "dateFinished": "2021-07-31 04:30:48.196",
      "status": "FINISHED",
      "errorMessage": "",
      "progressUpdateIntervalMs": 500
    },
    {
      "title": "Mean Dec proper motion plot",
      "text": "%pyspark\n\nplot.figure(2, figsize \u003d (16.18, 10.0))\n\narray_data \u003d np.empty(npix)\nfor item in df.rdd.collect():  array_data[item[0]] \u003d item[3]\nhp.mollview(array_data, fig\u003d2, coord\u003d\u0027C\u0027, unit\u003d\u0027mas/yr\u0027, nest\u003dTrue, title\u003d\u0027Mean Dec proper motion at HEALPix level %d\u0027%(healpix_level), cmap\u003d\u0027rainbow\u0027)\nhp.graticule(coord\u003d\u0027C\u0027, color\u003d\u0027white\u0027)\n",
      "user": "zrq",
      "dateUpdated": "2021-07-31 04:30:48.240",
      "config": {
        "colWidth": 12.0,
        "fontSize": 9.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "python",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "editorMode": "ace/mode/python",
        "title": true,
        "editorHide": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "apps": [],
      "jobName": "paragraph_1620645583560_-113720603",
      "id": "20210510-111943_814907111",
      "dateCreated": "2021-05-10 11:19:43.561",
      "dateStarted": "2021-07-31 04:30:48.722",
      "dateFinished": "2021-07-31 04:30:50.088",
      "status": "FINISHED",
      "errorMessage": "",
      "progressUpdateIntervalMs": 500
    },
    {
      "title": "Further reading and resources",
      "text": "%md\n\n* [Gaia source ID definition (for HEALPix indexing)](https://dms.cosmos.esa.int/COSMOS/doc_fetch.php?id\u003d2779219)\n* [Python package healpy](https://healpy.readthedocs.io/en/latest/index.html)\n* [Python matplotlib plotting library](https://matplotlib.org)\n* [Handy HEALPixel characteristics for various levels](https://lambda.gsfc.nasa.gov/toolbox/tb_pixelcoords.cfm)\n\n",
      "user": "zrq",
      "dateUpdated": "2021-07-31 04:30:50.123",
      "config": {
        "colWidth": 12.0,
        "fontSize": 9.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true,
          "completionKey": "TAB",
          "completionSupport": false
        },
        "editorMode": "ace/mode/markdown",
        "title": true,
        "editorHide": true,
        "tableHide": false
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "apps": [],
      "jobName": "paragraph_1620645596813_1492318238",
      "id": "20210510-111956_1822284967",
      "dateCreated": "2021-05-10 11:19:56.813",
      "dateStarted": "2021-07-31 04:30:50.288",
      "dateFinished": "2021-07-31 04:30:50.304",
      "status": "FINISHED",
      "errorMessage": "",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%md\n",
      "user": "zrq",
      "dateUpdated": "2021-07-31 04:30:50.387",
      "config": {
        "colWidth": 12.0,
        "fontSize": 9.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true,
          "completionKey": "TAB",
          "completionSupport": false
        },
        "editorMode": "ace/mode/markdown"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "apps": [],
      "jobName": "paragraph_1620653087219_-1084596777",
      "id": "20210510-132447_1514402898",
      "dateCreated": "2021-05-10 13:24:47.219",
      "status": "FINISHED",
      "errorMessage": "",
      "progressUpdateIntervalMs": 500
    }
  ],
  "name": "AglaisPublicExamples/Mean proper motions over the sky",
  "id": "2G748GZSW",
  "noteParams": {},
  "noteForms": {},
  "angularObjects": {
    "md:shared_process": [],
    "spark:gaiauser:": []
  },
  "config": {
    "isZeppelinNotebookCronEnable": false
  },
  "info": {}
}